---
title: "EDA & Forecasting"
output: html_notebook
---
  
## Setup    
  Load the relevant libraries.
```{r, message=FALSE, warning=FALSE}

# rm(list = ls())
# .rs.restartR()


# data manipulation
library("plyr")
library("tidyverse")
library("magrittr")
library("data.table")
library("lubridate")
library("sqldf")


# time series specific packages
library("timetk")
library("zoo")
library("tibbletime")


# modeling
library("fpp2")
# library("forecast")
library("prophet")
library("caret")
library("randomForest")
library("xgboost")
library("h2o")
# library("glmnet")
# library("neuralnet")
# library("nnet")
# library("gam")
# library("RANN")               # kNN (used in caret NA imputation)
library("keras")
# use_session_with_seed(123456789) # setting the seed to obtain reproducible results
# see https://keras.rstudio.com/articles/faq.html#how-can-i-obtain-reproducible-results-using-keras-during-development and https://cran.r-project.org/web/packages/keras/vignettes/faq.html
# can also re-enable gpu and parallel processing by using:  use_session_with_seed(42, disable_gpu = FALSE, disable_parallel_cpu = FALSE)


# other
library("geosphere")          # specific for distance calculations from lat-lon pairs
library("naniar")             # inspecting missing data

library("recipes")            # used in Keras modeling to design matrices
library("tfruns")             # used in Keras modeling for trainin runs
library("stringr")            # string manipulation
library("ggplot2")            # viz
library("rsample")            # rolling samples for validation stats
library("sweep")              # more easily pull out model statistics
library("doParallel")         # parallel processing
library("rlang")              # building functions
# library("tis")                # get holiday data
# 
# 
# library("tidytext")           # text manipulation
# library("topicmodels")        # topic modeling
# library("ldatuning")          # estimating the proper number of topics

```
  
    
  Session Info.
```{r}

sessionInfo()

```
  
    
  Setup the root directory.
```{r "setup", include = FALSE}

require("knitr")

opts_knit$set(root.dir = "/Users/mdturse/Desktop/Analytics/Chicago_El_Divvy/")

```
  
    
  Setting `wd` as the working directory.
```{r}

wd <- getwd()

wd

```
  
    
## Obtain Data  
    
  Note that the raw data were obtained and transformed into .Rds files in separate .R scripts. For full details, see [https://github.com/supermdat/Chicago_El_Divvy](https://github.com/supermdat/Chicago_El_Divvy).  
    
  Import the relevant .Rds files.
```{r}

rds_file_paths <-
  list.files(path = paste0(wd,
                           "/Data/Processed"
                           ),
             pattern = "\\.*.Rds",
             full.names = TRUE
             )

rds_file_paths


rds_file_names <-
  str_replace(string = rds_file_paths,
              pattern = ".*/",
              replacement = ""
              )

# not importing the intermediate weather data
index_not_weather <-
  str_detect(string = rds_file_paths, pattern = "^(?!.*data_weather.Rds)")

rds_files <-
  rds_file_paths[index_not_weather] %>% 
  map(~ readRDS(file = .x)
      )

names(rds_files) <- rds_file_names[index_not_weather]


pmap(.l = list(a = rds_files,
               b = names(rds_files)
               ),
     .f = function(a, b) {
       message(b)
       
       str(a)
       }
     )


rm(rds_file_names, rds_file_paths, index_not_weather)

```

    
## Distance Between El and Divvy  
  
  I think the proximity of a close Divvy station could affect El ridership, so here, I'll calculate:
  1. The distance between El and Divvy stations; and
  2. The number of Divvy stations that are "close" to an El station  
    
### "Crow Flies" Distance  
  
  First, I just get get the necessary data sets. **Note 1:** As data for Divvy stations regularly includes multiple latitude and longitude values for the same station, I will simply take the average of these values. **Note 2:** As data for Divvy stations regularly include multiple values for when a station began (`online_date`), I will simply take the oldest of these values. **Note 3:** The first Divvy station and trip occurred in June 2013, so to have complete data, I will begin analyses from July 2013 onward.
```{r}

el_stations_latlon <-
  rds_files$data_el_stations_format_vars.Rds %>% 
  select(#stop_id,
         map_id,
         lat,
         lon
         ) %>% 
  distinct() %>% 
  rename(el_stop_id = map_id,
         el_lat = lat,
         el_lon = lon
         ) %>% 
  mutate(temp_col_for_joining = 1)

message("el_stations_latlon")
str(el_stations_latlon)
# View(el_stations_latlon)


divvy_stations_latlon <-
  rds_files$divvy_data_stations.Rds %>% 
  group_by(id) %>% 
  summarise(divvy_lat = mean(latitude, na.rm = TRUE),
            divvy_lon = mean(longitude, na.rm = TRUE)
            ) %>% 
  ungroup() %>% 
  arrange(id)

# View(divvy_stations_latlon)
# View(rds_files$divvy_data_stations.Rds)

divvy_stations_online_date <-
  rds_files$divvy_data_stations.Rds %>% 
  group_by(id) %>% 
  summarise(divvy_online_date_min = min(online_date, na.rm = TRUE)
            ) %>% 
  ungroup() %>% 
  arrange(id)

# View(divvy_stations_online_date)

divvy_stations_latlon_online <-
  divvy_stations_latlon %>% 
  left_join(y = divvy_stations_online_date,
            by = "id"
            ) %>% 
  rename(divvy_station_id = id) %>% 
  mutate(temp_col_for_joining = 1)

# View(divvy_stations_latlon_online)
message("divvy_stations_latlon_online")
summary(divvy_stations_latlon_online)

message("divvy_stations_latlon_online$divvy_online_date_min")
summary(divvy_stations_latlon_online$divvy_online_date_min)
message("rds_files$data_divvy_trips.Rds$start_dt")
summary(rds_files$data_divvy_trips.Rds$start_dt)


rm(divvy_stations_latlon, divvy_stations_online_date)

```
  
    
  Now I simply get the unique combinations of El stations and Divvy stations into one dataset.
```{r}

stations_el_divvy_all <-
  el_stations_latlon %>% 
  inner_join(y = divvy_stations_latlon_online,
             by = "temp_col_for_joining"
             ) %>% 
  arrange(el_stop_id,
          divvy_station_id
          ) %>% 
  select(-temp_col_for_joining)

str(stations_el_divvy_all)
# View(stations_el_divvy_all %>% head(1000))


# rm(el_stations_latlon)
rm(divvy_stations_latlon_online)

```
  
    
  Here I calculate the Haversine ("crow flies") distance. This is not exactly the same as the walking distance (e.g., from Google Maps), but it should be similar, and this calculation avoids having to pay for using the Google Distance Matrix API).
```{r}

# user  system elapsed 
#  32.793   3.027  36.038
system.time(
distance_el_divvy_stations <-
  stations_el_divvy_all %>% 
  mutate(dist_miles = by(data = stations_el_divvy_all,
                         INDICES = 1:nrow(stations_el_divvy_all),
                         FUN = function(row) {
                           distHaversine(p1 = c(row$el_lon, row$el_lat),
                                         p2 = c(row$divvy_lon, row$divvy_lat)
                                         )
                           }
                         ) / 1609.344, # meters in 1 mile
         dist_miles = as.numeric(dist_miles), # this is done to force the variable to be numeric instead of 'by' numeric
         temp_col_for_joining = 1
         ) %>% 
  as.data.table() %>% 
  setkey(temp_col_for_joining,
         el_stop_id,
         divvy_online_date_min
         # dist_miles
         )
)
  
str(distance_el_divvy_stations)
summary(distance_el_divvy_stations)

# View(head(distance_el_divvy_stations, 1000))

rm(stations_el_divvy_all)

```
  
  
## Calculate a feature counting the number of Divvy Stations within X miles of every El Station (on every day)
  
    
  As some Divvy stations came online (came into use) on diffrent dates, here I manipulate the data to ensure combinations of El-Divvy stations are only counted after the Divvy station is in operation.  
    
  First, I create the dataframe of all dates used in the analyses (2013-07-01 to 2018-01-01).
```{r}

el_dates <-
  data.frame("el_date" = seq(from = ymd("2013-07-01"),
                             to = ymd("2018-01-01"),
                             by ="day"
                             )
             ) %>% 
  mutate(temp_col_for_joining = 1) %>% 
  as.data.table() %>% 
  setkey(temp_col_for_joining,
         el_date
         )

message("el_dates")
str(el_dates)

```
  
    
  Now, for those instances when the el_station--divvy_station distance is 0.5 miles or below, I create a dataset that incorporates these combinations with all `el_dates` dates (i.e., all the dates in the analyses).
```{r}

el_divvy_distance_setting <- 0.5

dist_miles_pt5 <-
  distance_el_divvy_stations[dist_miles <= el_divvy_distance_setting]

# View(dist_miles_pt5)
rm(el_divvy_distance_setting)


# user  system elapsed 
#   0.413   0.082   0.541
system.time(
  full_dates_dt <-
    merge(x = el_dates,
          # y = distance_el_divvy_stations,
          y = dist_miles_pt5,
          by.x = "temp_col_for_joining",
          by.y = "temp_col_for_joining",
          all = TRUE,
          allow.cartesian = TRUE
          ) %>% 
    setkey(el_stop_id,
           el_date,
           divvy_online_date_min
           )
  )

message("full_dates_dt")
str(full_dates_dt)
# View(full_dates_dt %>% tail(10000))
rm(dist_miles_pt5)

```
  
    
  Now I limit the dataset to only those `el_dates` that appear **AFTER** `divvy_online_date_min` (the date the Divvy station was made available).
```{r}

within_pt5miles_after_online <-
  full_dates_dt[ , c("temp_col_for_joining",
                     "el_lat",
                     "el_lon",
                     "divvy_lat",
                     "divvy_lon"
                     ) := NULL
                 ][el_date >= divvy_online_date_min
                   ][order(el_date,
                           el_stop_id,
                           dist_miles
                           )
                     ]

message("within_pt5miles_after_online")
str(within_pt5miles_after_online)
# View(within_pt5miles_after_online %>% filter(el_stop_id == 40090))
rm(full_dates_dt)

```
  
    
  Here, for each `el_stop_id` and `el_date`, I count the number of instances - which are the number of Divvy stations within 0.5 miles of the El station on that day
```{r}

within_pt5miles_cnts_el_stop_date <-
  within_pt5miles_after_online %>% 
  count(el_stop_id,
        el_date
        ) %>% 
  rename(num_divvy_stns_pt5_miles = n)

message("within_pt5miles_cnts_el_stop_date")
str(within_pt5miles_cnts_el_stop_date)
# View(within_pt5miles_cnts_el_stop_date %>% filter(el_stop_id == 40090))
# rm(within_pt5miles_after_online)

```
  
    
  For each unique combination of `el_stations` and `el_dates`, now I add in the number of Divvy stations within 0.5 miles (as of that date). This will be a variable used when modeling/forecasting.
```{r}

el_stations_all_dates <-
  el_dates %>% 
  inner_join(y = el_stations_latlon %>% select(el_stop_id, temp_col_for_joining),
             by = "temp_col_for_joining"
             ) %>% 
  arrange(el_stop_id,
          el_date
          ) %>% 
  select(el_stop_id,
         el_date
         )

message("el_stations_all_dates")
str(el_stations_all_dates)
# View(el_stations_all_dates %>% head(1000))
rm(el_dates, el_stations_latlon)



el_stns_full_dates_within_pt5 <-
  el_stations_all_dates %>% 
  left_join(y = within_pt5miles_cnts_el_stop_date,
            by = c("el_stop_id" = "el_stop_id",
                   "el_date" = "el_date"
                   )
            ) %>% 
  left_join(y = within_pt5miles_after_online,
            by = c("el_stop_id" = "el_stop_id",
                   "el_date" = "el_date"
                   )
            ) %>% 
  mutate(num_divvy_stns_pt5_miles = if_else(is.na(num_divvy_stns_pt5_miles),
                                            as.integer(0),
                                            num_divvy_stns_pt5_miles
                                            )
         ) %>% 
  as.data.table() %>% 
  setkey(el_stop_id,
         el_date
         )

message("el_stns_full_dates_within_pt5")
str(el_stns_full_dates_within_pt5)
# View(el_stns_full_dates_within_pt5 %>% filter(el_stop_id == 40090))
rm(within_pt5miles_cnts_el_stop_date)


## Save the original data to the proper folder
saveRDS(el_stns_full_dates_within_pt5,
        paste0(wd,
               "/Data/Interim/",
               "el_stns_full_dates_within_pt5.Rds"
               )
        )


# confirming the steps above worked
message("distance_el_divvy_stations")
str(distance_el_divvy_stations)
# View(distance_el_divvy_stations[el_stop_id == 40090 &
#                                   dist_miles <= 0.5
#                                 ][order(divvy_online_date_min)]
#      )


rm(within_pt5miles_after_online)

```
  
    
## Calculate a feature calculating the closest Divvy station to each El station (on every day)
**Note:** This must be done for each combination of date and El station as Divvy stations are only online (made available) after certain dates.  
  
  Because the combination of data leads to a very large data file, I use `purrr:pmap` to do the joining separatey for each `el_stop_id`, and then combine the results into a single data.table.  
    
  Here I create the data.tables to use - duplicate variables for `el_date2` and `divvy_online_date_min` are created for the purpose of merging the datasets.
```{r}

message("el_stations_all_dates")
str(el_stations_all_dates)
message("distance_el_divvy_stations")
str(distance_el_divvy_stations)

el_stations_all_dates_dt <-
  el_stations_all_dates %>% 
  mutate(el_date2 = el_date) %>% 
  as.data.table() %>% 
  setkey(el_stop_id,
         el_date2
         )

distance_el_divvy_stations_dt <-
  distance_el_divvy_stations[ , divvy_online_date_min2 := divvy_online_date_min] %>% 
  setkey(el_stop_id,
         divvy_online_date_min2
         )


## Save the original data to the proper folder
saveRDS(distance_el_divvy_stations_dt,
        paste0(wd,
               "/Data/Interim/",
               "distance_el_divvy_stations_dt.Rds"
               )
        )


message("el_stations_all_dates_dt")
str(el_stations_all_dates_dt)
message("distance_el_divvy_stations_dt")
str(distance_el_divvy_stations_dt)


rm(el_stations_all_dates, distance_el_divvy_stations)

```
  
    
  Now, using `purrr:pmap` I perform the merge for each value of `el_stop_id`, then I put the results together into a single data.table.
```{r}

## create the distinct values for el_stop_id
el_stns_dstnct <-
  el_stations_all_dates_dt %>% 
  select(el_stop_id) %>% 
  distinct() %>% 
  pull()


## the merge itself
# user  system elapsed 
# 108.265   4.020 113.568
system.time(
pmap_merge <-
  pmap(.l = list(a = el_stns_dstnct),
       .f = function(a) {
         # create small data.tables
         sngle_el_stn_all_dates = el_stations_all_dates_dt[el_stop_id == a]
         sngle_dstnc_el_divvy_stns = distance_el_divvy_stations_dt[el_stop_id == a]
         
         # the merge
         full_dt =
           sngle_dstnc_el_divvy_stns[sngle_el_stn_all_dates,
                                     on = .(el_stop_id = el_stop_id,
                                            divvy_online_date_min2 <= el_date2
                                            ),
                                     allow.cartesian = TRUE
                                     ]
         # [ ,
         #                               .(min_dist_miles = min(dist_miles)
         #                                 ),
         #                               keyby = c("el_stop_id", "el_date")
         #                               ]
         
         min_dt =
           full_dt %>% 
           group_by(el_stop_id,
                    el_date
                    ) %>% 
           top_n(n = -1, wt = dist_miles) %>% 
           ungroup() %>% 
           rename(min_dist_miles = dist_miles) %>% 
           select(el_stop_id,
                  el_date,
                  min_dist_miles,
                  divvy_station_id
                  ) %>% 
           as.data.table() %>% 
           setkey(el_stop_id,
                  el_date
                  )
         
         return(min_dt)
         }
       )
)


## create the single data.table
el_stns_full_dates_miles_to_closest_divvy <-
  bind_rows(pmap_merge) %>% 
  distinct() %>% 
  setkey(el_stop_id,
         el_date
         )

str(el_stns_full_dates_miles_to_closest_divvy)
# View(el_stns_full_dates_miles_to_closest_divvy[el_stop_id == 40090])


## Save the original data to the proper folder
saveRDS(el_stns_full_dates_miles_to_closest_divvy,
        paste0(wd,
               "/Data/Interim/",
               "el_stns_full_dates_miles_to_closest_divvy.Rds"
               )
        )


rm(el_stns_dstnct, pmap_merge, el_stations_all_dates_dt)

```
  
    
## Munge Divvy trips data into something more usable  
    
  Inspecting divvy trips by day. First, looking at groupings of the "from" station to the "to" station. These data are interesting, but look too infrequent to be very useful. Also, I don't believe the analyses at this level will vary greatly on where the trip was going to.
```{r}

message("rds_files$data_divvy_trips.Rds")
str(rds_files$data_divvy_trips.Rds)


divvy_trips_by_day_from_to <-
  rds_files$data_divvy_trips.Rds[ , start_date:= as_date(start_dt)
                                 ][ ,
                                   .(trips_cnt = .N,
                                     # trip_duration_mean = mean(tripduration, na.rm = TRUE),
                                     # trip_duration_med = median(tripduration, na.rm = TRUE),
                                     trip_duration_sum = sum(tripduration)
                                     ),
                                   keyby = c("start_date",
                                             "usertype",
                                             "from_station_id",
                                             "to_station_id"
                                             )
                                   ]

message("divvy_trips_by_day_from_to")
str(divvy_trips_by_day_from_to)
# View(tail(divvy_trips_by_day_from_to, 1000))


rm(divvy_trips_by_day_from_to)

```
  
    
  So here, I create counts, and `tripduration` stats based on the `start_date` and the `from_station_id`.
```{r}

divvy_trips_by_day_from_l <-
  rds_files$data_divvy_trips.Rds[ , start_date:= as_date(start_dt)
                                 ][ ,
                                   .(trip_cnt__dayfrom = .N,
                                     # trip_duration_mean__dayfrom =
                                     #   mean(tripduration, na.rm = TRUE),
                                     # trip_duration_med__dayfrom =
                                     #   median(tripduration, na.rm = TRUE),
                                     trip_duration_sum__dayfrom =
                                       sum(tripduration)
                                     ),
                                   keyby = c("start_date",
                                             "usertype",
                                             "from_station_id"
                                             )
                                   ]

message("divvy_trips_by_day_from_l")
str(divvy_trips_by_day_from_l)
# View(divvy_trips_by_day_from_l[from_station_id == 238])

```
  
    
  Munge the data into a "wider" format based on the `usertype` value.
```{r}

divvy_trips_by_day_from <-
  divvy_trips_by_day_from_l %>% 
  gather(key = variable,
         value = value,
         trip_cnt__dayfrom:trip_duration_sum__dayfrom
         ) %>% 
  unite(col = temp,
        usertype,
        variable,
        sep = "_"
        ) %>%
  spread(key = temp,
         value = value,
         fill = 0
         ) %>% 
  as.data.table() %>% 
  setkey(start_date,
         from_station_id
         )

message("divvy_trips_by_day_from")
str(divvy_trips_by_day_from)
# View(divvy_trips_by_day_from[from_station_id == 238])


## Save the data to the proper folder
saveRDS(divvy_trips_by_day_from,
        paste0(wd,
               "/Data/Interim/",
               "divvy_trips_by_day_from.Rds"
               )
        )


## Quick plot inspection
frequently_used_station <-
  divvy_trips_by_day_from %>% 
  top_n(n = 1, wt = Subscriber_trip_cnt__dayfrom) %>% 
  pull(from_station_id)

divvy_trips_by_day_from[from_station_id == frequently_used_station] %>% 
  ggplot(aes(x = start_date,
             y = Subscriber_trip_cnt__dayfrom
             )
         ) +
  geom_line()


rm(frequently_used_station, divvy_trips_by_day_from_l)

```

  And now I create counts, and `tripduration` stats based on the `start_date` (i.e., without considering `from_station_id`).
```{r}

divvy_trips_by_day_l <-
  rds_files$data_divvy_trips.Rds[ , start_date:= as_date(start_dt)
                                 ][ ,
                                   .(trip_cnt__day = .N,
                                     trip_duration_mean__day =
                                       mean(tripduration, na.rm = TRUE),
                                     trip_duration_med__day =
                                       median(tripduration, na.rm = TRUE)
                                     ),
                                   keyby = c("start_date",
                                             "usertype"
                                             )
                                   ]

message("divvy_trips_by_day_l")
str(divvy_trips_by_day_l)
# View(divvy_trips_by_day_l)

```
  
    
  As above, here I create a wider dataset vased on the `usertype` value.
```{r}

divvy_trips_by_day <-
  divvy_trips_by_day_l %>% 
  gather(key = variable,
         value = value,
         trip_cnt__day:trip_duration_med__day
         ) %>% 
  unite(col = temp,
        usertype,
        variable,
        sep = "_"
        ) %>%
  spread(key = temp,
         value = value,
         fill = 0
         ) %>% 
  as.data.table() %>% 
  setkey(start_date)

message("divvy_trips_by_day")
str(divvy_trips_by_day)
# View(divvy_trips_by_day)


## Save the data to the proper folder
saveRDS(divvy_trips_by_day,
        paste0(wd,
               "/Data/Interim/",
               "divvy_trips_by_day.Rds"
               )
        )


## Quick plot inspection
divvy_trips_by_day %>% 
  ggplot(aes(x = start_date,
             y = Subscriber_trip_cnt__day
             )
         ) +
  geom_line()


rm(divvy_trips_by_day_l)

```


## New Features

### Within 0.5 Miles, Create Station Counts, Trip Counts, and Trip Time Statistics  
  
  Original merge of the datasets.
```{r}

message("el_stns_full_dates_within_pt5")
str(el_stns_full_dates_within_pt5)
message("divvy_trips_by_day_from")
str(divvy_trips_by_day_from)
# View(divvy_trips_by_day_from[from_station_id == 238])


the_merge <-
  merge(x = el_stns_full_dates_within_pt5,
        y = divvy_trips_by_day_from,
        by.x = c("el_date", "divvy_station_id"),
        by.y = c("start_date", "from_station_id"),
        all.x = TRUE
        )

message("the_merge")
str(the_merge)
# View(the_merge[el_stop_id == 40090])

```
  
    
  Creating summed variables grouped by `el_date`, `el_stop_id`, and `num_divvy_stns_pt5_miles`
```{r}

new_vars_by <-
  the_merge[ ,
            .(c_tripcnt__dayfrom = sum(Customer_trip_cnt__dayfrom),
              c_tripduration_sum__dayfrom = sum(Customer_trip_duration_sum__dayfrom),
              d_tripcnt__dayfrom = sum(Dependent_trip_cnt__dayfrom),
              d_tripduration_sum__dayfrom = sum(Dependent_trip_duration_sum__dayfrom),
              s_tripcnt__dayfrom = sum(Subscriber_trip_cnt__dayfrom),
              s_tripduration_sum__dayfrom = sum(Subscriber_trip_duration_sum__dayfrom)
              ),
            keyby = c("el_date", "el_stop_id", "num_divvy_stns_pt5_miles")
            ]

str(new_vars_by)

```
  
    
  Creating a new variable for the mean trip duration, and if any new variable is NA, this means no trips, so change NA to 0 (zero).
```{r}

new_vars_1 <-
  new_vars_by[ ,
              `:=` (c_tripduration_mean__dayfrom =
                      c_tripduration_sum__dayfrom / c_tripcnt__dayfrom,
                    d_tripduration_mean__dayfrom =
                      d_tripduration_sum__dayfrom / d_tripcnt__dayfrom,
                    s_tripduration_mean__dayfrom =
                      s_tripduration_sum__dayfrom / s_tripcnt__dayfrom
                    )
              ]

message("new_vars_1")
str(new_vars_1)
rm(new_vars_by)



new_vars_2 <-
  new_vars_1[ ,
             `:=` (Customer_tripcnt__dayfrom =
                     if_else(num_divvy_stns_pt5_miles >= 0 &
                               is.na(c_tripcnt__dayfrom),
                             as.numeric(0),
                             c_tripcnt__dayfrom
                             ),
                   Customer_tripduration_mean__dayfrom =
                     if_else(num_divvy_stns_pt5_miles >= 0 &
                               is.na(c_tripduration_mean__dayfrom),
                             as.numeric(0),
                             c_tripduration_mean__dayfrom
                             ),
                   Dependent_tripcnt__dayfrom =
                     if_else(num_divvy_stns_pt5_miles >= 0 &
                               is.na(d_tripcnt__dayfrom),
                             as.numeric(0),
                             d_tripcnt__dayfrom
                             ),
                   Dependent_tripduration_mean__dayfrom =
                     if_else(num_divvy_stns_pt5_miles >= 0 &
                               is.na(d_tripduration_mean__dayfrom),
                             as.numeric(0),
                             d_tripduration_mean__dayfrom
                             ),
                   Subscriber_tripcnt__dayfrom =
                     if_else(num_divvy_stns_pt5_miles >= 0 &
                               is.na(s_tripcnt__dayfrom),
                             as.numeric(0),
                             s_tripcnt__dayfrom
                             ),
                   Subscriber_tripduration_mean__dayfrom =
                     if_else(num_divvy_stns_pt5_miles >= 0 &
                               is.na(s_tripduration_mean__dayfrom),
                             as.numeric(0),
                             s_tripduration_mean__dayfrom
                             )
                   )
             ]

message("new_vars_2")
str(new_vars_2)
rm(new_vars_1)

```
  
    
  Remove no-longer-needed variables and rename the kept variables.
```{r}

clean_vars <-
  new_vars_2[ , c("c_tripduration_sum__dayfrom",
                  "c_tripcnt__dayfrom",
                  "c_tripduration_mean__dayfrom",
                  "d_tripduration_sum__dayfrom",
                  "d_tripcnt__dayfrom",
                  "d_tripduration_mean__dayfrom",
                  "s_tripduration_sum__dayfrom",
                  "s_tripcnt__dayfrom",
                  "s_tripduration_mean__dayfrom"
                  ) := NULL
              ] %>% 
  rename(divvy_pt5mi_stn_cnt = num_divvy_stns_pt5_miles,
         divvy_pt5mi_trip_cnt_cus = Customer_tripcnt__dayfrom,
         divvy_pt5mi_triptime_mean_cus = Customer_tripduration_mean__dayfrom,
         divvy_pt5mi_trip_cnt_dep = Dependent_tripcnt__dayfrom,
         divvy_pt5mi_triptime_mean_dep = Dependent_tripduration_mean__dayfrom,
         divvy_pt5mi_trip_cnt_sub = Subscriber_tripcnt__dayfrom,
         divvy_pt5mi_triptime_mean_sub = Subscriber_tripduration_mean__dayfrom
         )

rm(new_vars_2)


# Confirm everything worked
# View(the_merge %>% filter(el_stop_id == 40090))
# View(clean_vars[el_stop_id == 40090])


message("clean_vars")
str(clean_vars)

message("el_stns_full_dates_within_pt5")
str(el_stns_full_dates_within_pt5)


rm(el_stns_full_dates_within_pt5, the_merge)

```
  

### Add in the general daily data (not by specific Divvy station) - i.e., the Divvy rides data for each day
```{r}

# View(clean_vars[el_stop_id == 40090])
# 
# str(divvy_trips_by_day)
# View(divvy_trips_by_day)


## initial merge
base_merge <-
  merge(x = clean_vars,
        y = divvy_trips_by_day,
        by.x = c("el_date"),
        by.y = c("start_date"),
        all.x = TRUE
        )

rm(clean_vars)


## change NA to zero
new_vars <-
  base_merge[ ,
             `:=` (divvy_all_trip_cnt_cus =
                     if_else(is.na(Customer_trip_cnt__day),
                             as.numeric(0),
                             Customer_trip_cnt__day
                             ),
                   divvy_all_triptime_mean_cus =
                     if_else(is.na(Customer_trip_duration_mean__day),
                             as.numeric(0),
                             Customer_trip_duration_mean__day
                             ),
                   divvy_all_triptime_med_cus =
                     if_else(is.na(Customer_trip_duration_med__day),
                             as.numeric(0),
                             Customer_trip_duration_med__day
                             ),
                   divvy_all_trip_cnt_dep =
                     if_else(is.na(Dependent_trip_cnt__day),
                             as.numeric(0),
                             Dependent_trip_cnt__day
                             ),
                   divvy_all_triptime_mean_dep =
                     if_else(is.na(Dependent_trip_duration_mean__day),
                             as.numeric(0),
                             Dependent_trip_duration_mean__day
                             ),
                   divvy_all_triptime_med_dep =
                     if_else(is.na(Dependent_trip_duration_med__day),
                             as.numeric(0),
                             Dependent_trip_duration_med__day
                             ),
                   divvy_all_trip_cnt_sub =
                     if_else(is.na(Subscriber_trip_cnt__day),
                             as.numeric(0),
                             Subscriber_trip_cnt__day
                             ),
                   divvy_all_triptime_mean_sub =
                     if_else(is.na(Subscriber_trip_duration_mean__day),
                             as.numeric(0),
                             Subscriber_trip_duration_mean__day
                             ),
                   divvy_all_triptime_med_sub =
                     if_else(is.na(Subscriber_trip_duration_med__day),
                             as.numeric(0),
                             Subscriber_trip_duration_med__day
                             )
                   )
             ]

rm(base_merge)
# str(new_vars)


## remove no-longer-needed variables
add_daily_data <-
  new_vars[ ,
           c("Customer_trip_cnt__day",
             "Customer_trip_duration_mean__day",
             "Customer_trip_duration_med__day",
             "Dependent_trip_cnt__day",
             "Dependent_trip_duration_mean__day",
             "Dependent_trip_duration_med__day",
             "Subscriber_trip_cnt__day",
             "Subscriber_trip_duration_mean__day",
             "Subscriber_trip_duration_med__day"
             ) := NULL] %>% 
  setkey(el_date,
         el_stop_id
         )

str(add_daily_data)
# View(add_daily_data[el_stop_id == 40090])
# View(divvy_trips_by_day)


rm(new_vars)

```


### Add data on trip counts and trip times for the closest Divvy station to each El station (on each day)  
  
  First merge daily data and data on the closest divvy stations.
```{r}

# str(el_stns_full_dates_miles_to_closest_divvy)
# View(el_stns_full_dates_miles_to_closest_divvy[el_stop_id == 40090])
# 
# str(divvy_trips_by_day_from)
# View(divvy_trips_by_day_from[from_station_id == 242])


merge_in_closest_divvy_stn <-
  merge(x = add_daily_data,
        y = el_stns_full_dates_miles_to_closest_divvy,
        by.x = c("el_date", "el_stop_id"),
        by.y = c("el_date", "el_stop_id"),
        all.x = TRUE
        )

message("add_daily_data")
str(add_daily_data)

message("merge_in_closest_divvy_stn")
str(merge_in_closest_divvy_stn)


rm(add_daily_data)

```
  
    
  Next merge in the data on Divvy stats by day and by Divvy station.
```{r}

# str(divvy_trips_by_day_from)

el_stns_full_dates_daily_stats_all <-
  merge(x = merge_in_closest_divvy_stn,
        y = divvy_trips_by_day_from,
        by.x = c("el_date", "divvy_station_id"),
        by.y = c("start_date", "from_station_id"),
        all.x = TRUE
        ) %>% 
  # rename the variables for consistency
  rename(divvy_mindist_miles = min_dist_miles,
         divvy_mindist_trip_cnt_cus = Customer_trip_cnt__dayfrom,
         divvy_mindist_triptime_sum_cus = Customer_trip_duration_sum__dayfrom,
         divvy_mindist_trip_cnt_dep = Dependent_trip_cnt__dayfrom,
         divvy_mindist_triptime_sum_dep = Dependent_trip_duration_sum__dayfrom,
         divvy_mindist_trip_cnt_sub = Subscriber_trip_cnt__dayfrom,
         divvy_mindist_triptime_sum_sub = Subscriber_trip_duration_sum__dayfrom
         ) %>% 
  mutate(divvy_mindist_trip_cnt_cus =
           if_else(is.na(divvy_mindist_trip_cnt_cus),
                   as.numeric(0),
                   divvy_mindist_trip_cnt_cus
                   ),
         divvy_mindist_triptime_mean_cus =
           if_else(is.na(divvy_mindist_triptime_sum_cus / divvy_mindist_trip_cnt_cus),
                   as.numeric(0),
                   divvy_mindist_triptime_sum_cus / divvy_mindist_trip_cnt_cus
                   ),
         divvy_mindist_trip_cnt_dep =
           if_else(is.na(divvy_mindist_trip_cnt_dep),
                   as.numeric(0),
                   divvy_mindist_trip_cnt_dep
                   ),
         divvy_mindist_triptime_mean_dep =
           if_else(is.na(divvy_mindist_triptime_sum_dep / divvy_mindist_trip_cnt_dep),
                   as.numeric(0),
                   divvy_mindist_triptime_sum_dep / divvy_mindist_trip_cnt_dep
                   ),
         divvy_mindist_trip_cnt_sub =
           if_else(is.na(divvy_mindist_trip_cnt_sub),
                   as.numeric(0),
                   divvy_mindist_trip_cnt_sub
                   ),
         divvy_mindist_triptime_mean_sub =
           if_else(is.na(divvy_mindist_triptime_sum_sub / divvy_mindist_trip_cnt_sub),
                   as.numeric(0),
                   divvy_mindist_triptime_sum_sub / divvy_mindist_trip_cnt_sub
                   )
         ) %>% 
  select(-divvy_mindist_triptime_sum_cus,
         -divvy_mindist_triptime_sum_dep,
         -divvy_mindist_triptime_sum_sub,
         -divvy_station_id
         ) %>% 
  as.data.table() %>% 
  setkey(el_date,
         el_stop_id
         )


message("merge_in_closest_divvy_stn")
str(merge_in_closest_divvy_stn)

message("el_stns_full_dates_daily_stats_all")
str(el_stns_full_dates_daily_stats_all)

# View(el_stns_full_dates_daily_stats_all[el_stop_id == 40090])

rm(merge_in_closest_divvy_stn, el_stns_full_dates_miles_to_closest_divvy)


## Save the data to the proper folder
saveRDS(el_stns_full_dates_daily_stats_all,
        paste0(wd,
               "/Data/Interim/",
               "el_stns_full_dates_daily_stats_all.Rds"
               )
        )

```


## Create the full dataset by merging in the additional data  
    
  Merge in data on el rides.
```{r}

message("el_stns_full_dates_daily_stats_all")
str(el_stns_full_dates_daily_stats_all)

message("rds_files$data_el_format_vars.Rds")
str(rds_files$data_el_format_vars.Rds)


## Quick plot inspection
freqently_used_el <-
  rds_files$data_el_format_vars.Rds %>% 
  top_n(n = 1, w = rides) %>% 
  pull(station_id)
  

rds_files$data_el_format_vars.Rds[station_id == freqently_used_el &
                                    date >= as_date("2013-07-01")
                                  ] %>% 
  ggplot(aes(x = date,
             y = rides
             )
         ) +
  geom_line()
  

## The merge
add_el_rides <-
  merge(x = el_stns_full_dates_daily_stats_all,
        y = rds_files$data_el_format_vars.Rds,
        by.x = c("el_date", "el_stop_id"),
        by.y = c("date", "station_id"),
        all.x = TRUE
        ) %>% 
  rename(el_rides = rides) %>% 
  select(-daytype,
         -stationname
         )

message("el_stns_full_dates_daily_stats_all")
dim(el_stns_full_dates_daily_stats_all)

message("add_el_rides")
dim(add_el_rides)
str(add_el_rides)


rm(freqently_used_el, el_stns_full_dates_daily_stats_all)

```
  
    
  Merge in data on holidays.
```{r}

message("rds_files$data_holidays_format_vars.Rds")
str(rds_files$data_holidays_format_vars.Rds)


## rds_files$data_holidays_format_vars.Rds has one duplicate date, so remove that first
dupe_dates <-
  rds_files$data_holidays_format_vars.Rds %>% 
  count(date) %>% 
  arrange(desc(n)) %>% 
  filter(n != 1) %>% 
  pull(date)

remove_dupe <-
  rds_files$data_holidays_format_vars.Rds %>% 
  filter(date == as_date(dupe_dates)
         ) %>% 
  rowid_to_column() %>% 
  filter(rowid != 1) %>% 
  mutate(filter_temp = paste(date,
                             holiday_name,
                             holiday_comment,
                             sep = "_"
                             )
         )

holiday_dedupe <-
  rds_files$data_holidays_format_vars.Rds %>% 
  mutate(filter_temp = paste(date,
                             holiday_name,
                             holiday_comment,
                             sep = "_"
                             )
         ) %>% 
  filter(filter_temp != remove_dupe$filter_temp) %>% 
  select(-filter_temp)

message("holiday_dedupe")
str(holiday_dedupe)


## merge itself
base <-
  merge(x = add_el_rides,
        y = holiday_dedupe,
        by.x = "el_date",
        by.y = "date",
        all.x = TRUE
        )


## add 1/0 holiday indicator
add_holidays <-
  base[ , holiday := if_else(is.na(holiday_name), FALSE, TRUE)]


message("add_el_rides")
dim(add_el_rides)

message("add_holidays")
dim(add_holidays)
str(add_holidays)


rm(dupe_dates, remove_dupe, base, holiday_dedupe, add_el_rides)

```
  
    
  Merge in data on weather.
```{r}

message("rds_files$data_weather_format_vars.Rds")
str(rds_files$data_weather_format_vars.Rds)


## The merge
add_weather <-
  merge(x = add_holidays,
        y = rds_files$data_weather_format_vars.Rds,
        by.x = "el_date",
        by.y = "date",
        all.x = TRUE
        ) %>% 
  select(-station,
         -latitude,
         -longitude,
         -elevation,
         -name
         ) %>% 
  setkey(el_date,
         el_stop_id
         )


message("add_holidays")
dim(add_holidays)

message("add_weather")
dim(add_weather)
str(add_weather)


rm(add_holidays)
rm(distance_el_divvy_stations_dt, divvy_trips_by_day, divvy_trips_by_day_from)
# rm(rds_files)

```
  
    
  Create time-based variables, and add them in.  
    
  Here I use `timetk::tk_augment_timeseries_signature` to create the "time variables" (e.g., month, week, etc.), and I turn integer variables into factors to facilitate modeling later on.
```{r}

date_min <- min(add_weather$el_date)
date_max <- max(add_weather$el_date)

dates_dt <- 
  data.frame(base_date = as_date(date_min:date_max)
             ) %>%  
  tk_augment_timeseries_signature() %>% 
  select(-diff,
         -index.num,
         -year.iso,
         -month.xts,
         -month.lbl,
         -hour,
         -minute,
         -second,
         -hour12,
         -am.pm,
         -wday,
         -wday.xts,
         -mday,
         -qday,
         -yday,
         -week2,
         -week3,
         -week4,
         -mday7,
         -week,
         -week.iso
         ) %>% 
  mutate_if(is.integer, factor) %>% 
  as.data.table() %>% 
  setkey(base_date)
  

str(dates_dt)


rm(date_min, date_max)

```
  
    
  Then I simply merge the datasets.
```{r}

add_time <-
  merge(x = add_weather,
        y = dates_dt,
        by.x = "el_date",
        by.y = "base_date",
        all.x = TRUE
        ) %>% 
  setkey(el_stop_id,
         el_date)

dim(add_weather)
dim(add_time)

str(add_time)


rm(add_weather, dates_dt)

```


## Inspect Missing Values
```{r}

summary(add_time)
miss_var_summary(add_time)
# View(miss_var_summary(add_time))

```


### El Rides  
  
  I believe `NA` values for `el_rides` are for dates when a particular station was out of service (e.g., a station was out of service for a period of time, or only came into existence after a certain date). For modeling, I therefore limit the dataset to only those instances where `el_rides` is greater than zero. **NOTE:** For time series methods requiring equal spaced events, this "could" present an issue.  
    
  Showing an example of missing `el_rides` values
```{r}

el_stop_na <-
  add_time %>% 
  filter(is.na(el_rides)
         ) %>% 
  select(el_stop_id) %>% 
  arrange(el_stop_id) %>% 
  distinct() %>% 
  top_n(n = 2, wt = el_stop_id) %>% 
  pull(el_stop_id)


# View(
  add_time %>% 
    filter(el_stop_id %in% el_stop_na) %>% 
    select(el_stop_id,
           el_date,
           el_rides
           ) %>% 
    arrange(el_stop_id,
            el_date
            ) %>% 
    split(.$el_stop_id) %>% 
    map(~ head(.x, 100)
        )
  # )

rm(el_stop_na)

```
  
    
  Filter the data for `el_rides` greater than zero.
```{r}

el_rides_nozero_noNA <-
  add_time %>% 
  filter(el_rides >= 0)


message("rows removed")
nrow(add_time) - nrow(el_rides_nozero_noNA)


message("el_rides_nozero_noNA")
str(el_rides_nozero_noNA)
summary(el_rides_nozero_noNA)

miss_var_summary(el_rides_nozero_noNA)
# View(miss_var_summary(el_rides_nozero_noNA))


rm(add_time)

```


### Weather Data  
  
  Data were included for `el_rides` for dates up to, and including, 2018-01-01, but the Weather data stops at 2017-12-31. So I can simply remove data after 2017-12-31 (which removes the bulk of the NA Weather data).
```{r}

el_dates_no_weather <-
  el_rides_nozero_noNA %>% 
  filter(is.na(prcp)
         ) %>% 
  select(el_date) %>% 
  distinct() %>% 
  pull(el_date)


weather_20171231 <-
  el_rides_nozero_noNA %>% 
  filter(el_date < as_date(el_dates_no_weather)
         )


message("rows removed")
nrow(el_rides_nozero_noNA) - nrow(weather_20171231)

message("weather_20171231")
# str(weather_20171231)
# summary(weather_20171231)

miss_var_summary(weather_20171231)
# View(miss_var_summary(weather_20171231))


rm(el_dates_no_weather, el_rides_nozero_noNA)

```
  
    
  As `snwd` and `snow` appear to be sporadically (and rarely) missing (equipment failure?), here I simply fill in the `NA` values with the value from the previous day.
```{r}

weather_20171231 %>% 
  filter(is.na(snwd) | is.na(snow)
         ) %>% 
  select(el_date, snwd, snow) %>% 
  distinct()


data_no_missing <-
  weather_20171231 %>% 
  mutate(snwd = na.locf(snwd),
         snow = na.locf(snow)
         ) %>% 
  as.data.table() %>% 
  setkey(el_date,
         el_stop_id
         )


message("data_no_missing")
str(data_no_missing)
summary(data_no_missing)

miss_var_summary(data_no_missing)
# View(miss_var_summary(data_no_missing))


## Save the  data to the proper folder
saveRDS(data_no_missing,
        paste0(wd,
               "/Data/Interim/",
               "data_no_missing.Rds"
               )
        )


rm(weather_20171231)

```


## Feature Engineering

### Weather Variables
  
  As the prediction forcast is 7 days in the future, I don't think we'll be able to accuratley know the temperature variables `tmin` and `tmax` within one degree of accuracy. So here, I simply test if knowing within `X` degrees of accuracy is viable.  
    
  Visually, it looks like 25 degrees may be a good range (this is a lot wider than I epected!).
```{r}

func_temp_linegraph <-
  function(date_1, date_2) {
    data_no_missing %>% 
      select(el_date,
             tmin,
             tmax
             ) %>% 
      distinct() %>% 
      mutate(wk_start = floor_date(x = el_date,
                                   unit = "week"#,
                                   # week_start = getOption("lubridate.week.start", 7)
                                   )
             ) %>% 
      filter(dplyr::between(wk_start,
                            as_date(date_1),
                            as_date(date_2)
                            )
             ) %>% 
      gather(key = temp_type,
             value = degrees_f,
             tmin:tmax
             ) %>% 
      ggplot(aes(x = el_date,
                 y = degrees_f,
                 color = temp_type
                 )
             ) +
      geom_line() +
      facet_wrap(~ wk_start,
                 scales = "free",
                 ncol = 4
                 ) +
      theme_minimal() +
      NULL
  }

func_temp_linegraph(date_1 = "2013-06-30", date_2 = "2013-09-30")
func_temp_linegraph(date_1 = "2013-09-30", date_2 = "2013-12-31")
func_temp_linegraph(date_1 = "2013-12-31", date_2 = "2014-03-30")
func_temp_linegraph(date_1 = "2014-04-30", date_2 = "2014-06-30")
func_temp_linegraph(date_1 = "2014-06-30", date_2 = "2014-09-30")


rm(func_temp_linegraph)

```
  
    
  Formally testing accuracy for rounding to nearest 10 degrees with a lag of 7 days.
```{r}

min(data_no_missing$tmin)
max(data_no_missing$tmin)
min(data_no_missing$tmax)
max(data_no_missing$tmax)


temp_round <-
  data_no_missing %>% 
  select(el_date,
         tmin,
         tmax
         ) %>% 
  distinct() %>% 
  mutate(#tmin_rnd10 = floor(tmin / 10) * 10,
         #tmax_rnd10 = floor(tmax / 10) * 10,
         tmin_rnd10 = round(tmin, -1),
         tmax_rnd10 = round(tmax, -1),
         tmin_rnd10_l7 = dplyr::lag(x = tmin_rnd10, n = 7),
         tmax_rnd10_l7 = dplyr::lag(x = tmax_rnd10, n = 7)
         ) %>% 
  mutate_at(vars(tmin_rnd10:tmax_rnd10_l7), factor)

message("temp_round")
str(temp_round)
# View(temp_round)


message("tmin accuracy round")
temp_round %>% 
  select(tmin_rnd10,
         tmin_rnd10_l7
         ) %>% 
  filter(!is.na(tmin_rnd10_l7)) %>% 
  rename(obs = tmin_rnd10,
         pred = tmin_rnd10_l7
         ) %>% 
  multiClassSummary(lev = levels(temp_round$tmin_rnd10)
                    )


message("tmax accuracy round")
temp_round %>% 
  select(tmax_rnd10,
         tmax_rnd10_l7
         ) %>% 
  filter(!is.na(tmax_rnd10_l7)) %>% 
  rename(obs = tmax_rnd10,
         pred = tmax_rnd10_l7
         ) %>% 
  multiClassSummary(lev = levels(temp_round$tmax_rnd10)
                    )

```
  
    
  Formally testing accuracy for a band of 25 degrees with a lag of 7 days.
```{r}

temp_bands <-
  data_no_missing %>% 
  select(el_date,
         tmin,
         tmax
         ) %>% 
  distinct() %>% 
  mutate(tmin_bands = case_when(tmin < -25 ~ "01_blw_-25",
                                -25 <= tmin & tmin < 0 ~ "02_-25to00",
                                0 <= tmin & tmin < 25 ~ "03_00to25",
                                25 <= tmin & tmin < 50 ~ "04_25to50",
                                50 <= tmin & tmin < 75 ~ "05_50to75",
                                75 <= tmin & tmin < 100 ~ "06_75to100",
                                100 <= tmin ~ "07_abv_100"
                                ) %>% as.factor(),
         tmax_bands = case_when(tmax < 0 ~ "01_blw_00",
                                0 <= tmax & tmax < 25 ~ "02_00to25",
                                25 <= tmax & tmax < 50 ~ "03_25to50",
                                50 <= tmax & tmax < 75 ~ "04_50to75",
                                75 <= tmax & tmax < 100 ~ "05_75to100",
                                100 <= tmax ~ "06_abv_100"
                                ) %>% as.factor(),
         tmin_bands_l7 = dplyr::lag(tmin_bands, n = 7),
         tmax_bands_l7 = dplyr::lag(tmax_bands, n = 7)
         )

message("temp_bands")
str(temp_bands)
# View(temp_bands)


message("tmin accuracy bands")
temp_bands %>% 
  select(tmin_bands,
         tmin_bands_l7
         ) %>% 
  filter(!is.na(tmin_bands_l7)) %>% 
  rename(obs = tmin_bands,
         pred = tmin_bands_l7
         ) %>% 
  multiClassSummary(lev = levels(temp_bands$tmin_bands)
                    )


message("tmax accuracy bands")
temp_bands %>% 
  select(tmax_bands,
         tmax_bands_l7
         ) %>% 
  filter(!is.na(tmax_bands_l7)) %>% 
  rename(obs = tmax_bands,
         pred = tmax_bands_l7
         ) %>% 
  multiClassSummary(lev = levels(temp_bands$tmax_bands)
                    )

```
  
    
  As the 25 degree band does appear more accurate (although less accurate than expected), here I merely formalize the inclusion of the temperature bands.
```{r}

# create bands of temp
temp_bands_25F <-
  data_no_missing[ ,
                  `:=` (tmin_bands = case_when(tmin < -25 ~ "01_blw_-25",
                                               -25 <= tmin & tmin < 0 ~ "02_-25to00",
                                               0 <= tmin & tmin < 25 ~ "03_00to25",
                                               25 <= tmin & tmin < 50 ~ "04_25to50",
                                               50 <= tmin & tmin < 75 ~ "05_50to75",
                                               75 <= tmin & tmin < 100 ~ "06_75to100",
                                               100 <= tmin ~ "07_abv_100"
                                               ) %>% 
                          as.factor(),
                        tmax_bands = case_when(tmax < 0 ~ "01_blw_00",
                                               0 <= tmax & tmax < 25 ~ "02_00to25",
                                               25 <= tmax & tmax < 50 ~ "03_25to50",
                                               50 <= tmax & tmax < 75 ~ "04_50to75",
                                               75 <= tmax & tmax < 100 ~ "05_75to100",
                                               100 <= tmax ~ "06_abv_100"
                                               ) %>% 
                          as.factor()
                        )
                  ]

# create lags at 7 days
temp_bands_25F <-
  temp_bands_25F[ ,
                 `:=` (tmin_bands_l7 = dplyr::lag(tmin_bands, n = 7),
                       tmax_bands_l7 = dplyr::lag(tmax_bands, n = 7)
                       )
                 ]

# remove no-longer-needed vars
temp_bands_25F <-
  temp_bands_25F[ ,
                 c("tmin", "tmax", "tobs") := NULL
                 ]


str(temp_bands_25F)


rm(data_no_missing, temp_bands, temp_round)

```
  
    
  Visually, investigating precipitation-related variables (`prcp`, `snow`, and `snwd`).
```{r}

func_prcp_linegraph <-
  function(date_1, date_2) {
    temp_bands_25F %>% 
      select(el_date,
             snwd,
             snow,
             prcp
             ) %>% 
      distinct() %>% 
      mutate(wk_start = floor_date(x = el_date,
                                   unit = "week"
                                   )
             ) %>% 
      filter(dplyr::between(wk_start,
                            as_date(date_1),
                            as_date(date_2)
                            )
             ) %>% 
      gather(key = metric,
             value = value,
             snwd:prcp
             ) %>% 
      ggplot(aes(x = el_date,
                 y = value,
                 color = metric
                 )
             ) +
      geom_line() +
      # coord_flip() +
      facet_wrap(vars(metric, wk_start),
                 scales = "free",
                 ncol = 4#,
                 # labeller = "label_both"
                 ) +
      theme_minimal() +
      theme(legend.position = "none") +
      NULL
    }


func_prcp_linegraph(date_1 = "2015-02-01", date_2 = "2015-02-28")
func_prcp_linegraph(date_1 = "2015-04-01", date_2 = "2015-04-30")
func_prcp_linegraph(date_1 = "2015-06-01", date_2 = "2015-06-30")
func_prcp_linegraph(date_1 = "2015-08-01", date_2 = "2015-08-31")
func_prcp_linegraph(date_1 = "2015-10-01", date_2 = "2015-10-31")
func_prcp_linegraph(date_1 = "2015-12-01", date_2 = "2015-12-31")


rm(func_prcp_linegraph)

```
  
    
  The precipitation variables very likely do have predictive power, but being able to accurately predict them 7 days in advance is difficult. As there doesn't appear to be any easily/quickly identifiable pattern to produce seven-day-forward estimates for these variables, I merely delete them.
```{r}

remove_prcp_vars <-
  temp_bands_25F[ ,
             c("prcp", "snow", "snwd") := NULL
             ]


## Save the data to the proper folder
saveRDS(remove_prcp_vars,
        paste0(wd,
               "/Data/Interim/",
               "remove_prcp_vars.Rds"
               )
        )

# remove_prcp_vars <-
#   readRDS(paste0(wd,
#                  "/Data/Interim/",
#                  "remove_prcp_vars.Rds"
#                  )
#           )


str(remove_prcp_vars)


rm(temp_bands_25F)

```


### Divvy Data  
  
  As the desire is to predict `el_rides` for each level of `el_stop_id` for 7 days in the future. As a proof of concept, the remainder of the analyses will be done for 6 values of `el_stop_id`: the two most frequently used el stations (by el ridership), the two least frquently used el stations, and two el stations in the middle. This is necessary to do **before** feature engineering the divvy data, as the divvy data are specific to each value of `el_stop_id`.
    
  First, I just create a datset ranking `el_stop_id` by total el rides.
```{r}

el_stop_id_rank <-
  remove_prcp_vars %>% 
  group_by(el_stop_id) %>% 
  summarise(el_rides_sum = sum(el_rides),
            date_min = min(el_date),
            date_max = max(el_date)
            ) %>% 
  arrange(el_rides_sum) %>% 
  mutate(days_in_use = date_max - date_min,
         row_num = row_number(el_rides_sum)
         )

```
  
    
  So now I can just get the top, bottom, and middle to values for `el_station_id`.
```{r}

# top
el_stop_id_top2 <-
  el_stop_id_rank %>% 
  top_n(n = 2, wt = el_rides_sum) %>% 
  pull(el_stop_id)


# bottom
el_stop_id_bot2 <-
  el_stop_id_rank %>% 
  top_n(n = -2, wt = el_rides_sum) %>% 
  pull(el_stop_id)


# middle
mid_row_num <-
  round(max(el_stop_id_rank$row_num) / 2, digits = 0)

el_stop_id_mid2 <-
  el_stop_id_rank %>% 
  filter(row_num %in% c(mid_row_num, mid_row_num + 1)
         ) %>% 
  pull(el_stop_id)

rm(mid_row_num)


# put everything together
el_stop_id_6_values <-
  c(el_stop_id_bot2, el_stop_id_mid2, el_stop_id_top2)


rm(el_stop_id_rank, el_stop_id_bot2, el_stop_id_mid2, el_stop_id_top2)

```
  
    
  Now I can properly limit the data to the relevant `el_stop_id` values, and take 7-day lags of the relevant divvy variables.
```{r}

# limit the data to the six values of interest
six_stations_data <-
  el_stop_id_6_values %>% 
  map(~ filter(remove_prcp_vars,
               el_stop_id == .x
               ) %>% 
        arrange(el_date)
      )

six_stations_data %>% 
  map(~ dim(.x))
      
      
# get the proper lags
regex_criteria <- "_cus$|_dep$|sub$"

divvy_vars_lag7 <-
  six_stations_data %>% 
  map(~ select(.x,
               matches(regex_criteria)
               ) %>% 
        mutate_all(funs(l7 = dplyr::lag(., n = 7)
                        )
                   )
      )

names(divvy_vars_lag7) <- el_stop_id_6_values


pmap(.l = list(a = divvy_vars_lag7,
               b = names(divvy_vars_lag7)
               ),
     .f = function(a, b) {
       message(b)
       
       str(a)
       }
     )

# divvy_vars_lag7 %>% map(~ View(.x))

```
  
    
  As a check, here I investigate just how accurate using a 7-day lag is for each variable.  
    
  First I'll create the function `func_sum_stats_by_col` to calculate the accuracy statistics for each variable-pair (e.g., `divvy_pt5mi_trip_cnt_cus` and `divvy_pt5mi_trip_cnt_cus_l7`).
```{r}

func_sum_stats_by_col <-
  function(data) {
    dat = data[8:nrow(data), ]
    
    preProcValues = preProcess(dat, method = c("center", "scale"))
    dat_cent_scale = predict(preProcValues, dat)
    
    col_pos = seq(1:21)
    
    sum_stats =
      pmap(.l = list(a = col_pos),
           .f = function(a) {
             
             colnames(dat_cent_scale)[a] = "obs"
             colnames(dat_cent_scale)[a + 21] = "pred"
             
             stats = defaultSummary(dat_cent_scale)
             
             return(stats)
             }
           )
    
    return(sum_stats)
    }

```
  
    
  Now I simply use the `func_sum_stats_by_col` function for data on each value of `el_stop_id`, then turn the results into a dataframe for easier understanding.
```{r warning=FALSE}

acrcy_stats_lags_df <-
  pmap(.l = list(a = divvy_vars_lag7),
       .f = function(a) {
         res = func_sum_stats_by_col(a)
         
         names(res) = names(a)[1:21]
         
         res_df = bind_rows(res) %>% 
           t() %>% 
           as.data.frame() %>% 
           rename(RMSE = V1,
                  rSqrd = V2,
                  MAE = V3
                  ) %>% 
           rownames_to_column(var = "var_og")
         
         return(res_df)
         }
       )

# View(acrcy_stats_lags_df$`40600`)


rm(func_sum_stats_by_col)

```
  
    
  Now I just put all of the six datasets together as one, and then calculate some summary statistics.  
    
  Overall, using a lag of 7 has varying results depending on the variable. For example, using the median of the six MAE values, `divvy_pt5mi_triptime_mean_cus` is fairly accurate, whereas `divvy_all_triptime_mean_cus`. Interestingly, all of the values for `...pt5mi...` are the most accurate. Hopefully, this accuracy will be sufficient for predicing `el_rides`.
```{r}

acrcy_stats_lag_1df <-
  bind_rows(acrcy_stats_lags_df,
            .id = "el_stop_id"
            ) %>% 
  arrange(var_og,
          MAE,
          el_stop_id
          )

# View(acrcy_stats_lag_1df)


acrcy_stats_sum_stats <-
  acrcy_stats_lag_1df %>% 
  group_by(var_og) %>% 
  summarise(rmse_mean = mean(RMSE, na.rm = TRUE),
            rmse_med = median(RMSE, na.rm = TRUE),
            rsqrd_mean = mean(rSqrd, na.rm = TRUE),
            rsqrd_med = median(rSqrd, na.rm = TRUE),
            mae_mean = mean(MAE, na.rm = TRUE),
            mae_med = median(MAE, na.rm = TRUE)
            )

acrcy_stats_sum_stats %>% 
  arrange(mae_med)

# View(acrcy_stats_sum_stats %>% 
#        arrange(mae_med)
#      )


rm(acrcy_stats_lags_df, acrcy_stats_lag_1df, acrcy_stats_sum_stats)

```
  
    
  Now I simply add the lagged variables back into the "main" datasets in place of the un-lagged variables.
```{r}

# get the lagged variables
divvy_vars_lag7_lag_vars <-
  divvy_vars_lag7 %>% 
  map(~ select(.x,
               matches("_l7$")
               )
      )


# then bind them to the "main" data
six_stations_divvy_data_lags <-
  pmap(.l = list(a = six_stations_data,
                 b = divvy_vars_lag7_lag_vars
                 ),
       .f = function(a, b) {
         new_dat = a %>% 
           select(-matches(regex_criteria)
                  ) %>% 
           bind_cols(b)
         
         return(new_dat)
         }
       )

names(six_stations_divvy_data_lags) <- names(divvy_vars_lag7_lag_vars)


ncol(remove_prcp_vars)
ncol(six_stations_divvy_data_lags$`41140`)
colnames(six_stations_divvy_data_lags$`41140`)

# View(six_stations_divvy_data_lags$`41140`)


rm(remove_prcp_vars, divvy_vars_lag7, divvy_vars_lag7_lag_vars, six_stations_data, regex_criteria)

```


### Holidays

  Here I simply take the 7-day lag of the holiday-related variables, and turn relace `NA` values in `holiday_name` with a factor level for "not a holiday".
```{r}

six_stations_all_lags <-
  pmap(.l = list(a = six_stations_divvy_data_lags),
       .f = function(a) {
         dat = a %>% 
           mutate(holiday_name = if_else(is.na(holiday_name),
                                         "--Not_Holiday--",
                                         as.character(holiday_name)
                                         ) %>% factor,
                  holiday_name_l7 = dplyr::lag(holiday_name, n = 7),
                  holiday_comment_l7 = dplyr::lag(holiday_comment, n = 7),
                  holiday_l7 = dplyr::lag(holiday, n = 7)
                   )
         
         # dat = dat[8:nrow(dat), ]
         
         return(dat)
         }
      )


str(six_stations_all_lags$`41140`)
# View(six_stations_all_lags$`41140`)


rm(six_stations_divvy_data_lags, el_stop_id_6_values)

```


### El Rides

 Determining the optimal number of lags of `el_rides`to include (based on the ACF).
```{r}

# function to measure ACF for each lag
func_tidy_acf <- function(data, value, lags = 0:(7 * 8) # 7 days * 8 weeks as the default
                     ) {
  value_expr = enquo(value)
  
  acf_values = data %>% 
    pull(!!value_expr) %>% 
    acf(lag.max = tail(lags, 1),
        plot = FALSE
        ) %>% 
    .$acf %>% 
    .[,,1]
  
  ret = tibble(acf = acf_values) %>% 
    rowid_to_column(var = "lag") %>% 
    mutate(lag = lag - 1) %>% 
    filter(lag %in% lags)
  
  return(ret)
}



max_lag <- 7 * 52 * 2 # 7 days * 52 weeks * 2 years

pmap(.l = list(a = six_stations_all_lags,
               b = names(six_stations_all_lags)
               ),
     .f = function(a, b) {
       func_tidy_acf(a,
                     value = el_rides,
                     lags = 0:max_lag
                     ) %>% 
         # limited in this way as lags of 7n appear to be the most relevant
         filter(lag == 0 |
                  lag %% 7 == 0
                ) %>%
         mutate(url = b)
       }
     )


pmap(.l = list(a = six_stations_all_lags,
               b = names(six_stations_all_lags)
               ),
     .f = function(a, b) {
       func_tidy_acf(a,
                     value = el_rides,
                     lags = 0:max_lag
                     ) %>% 
         # limited in this way as lags of 7n appear to be the most relevant
         filter(lag == 0 |
                  lag %% 7 == 0
                ) %>%
         ggplot(aes(x = lag,
                    y = acf
                    )
                ) +
         geom_segment(aes(xend = lag,
                          yend = 0
                          ),
                      color = "blue"
                      ) +
         geom_vline(xintercept = 7,
                    size = 2,
                    color = "red"
                    ) +
         annotate("text", label = "1 Week Mark",
                  x = 18,
                  y = 0.9,
                  color = "red",
                  size = 3,
                  hjust = 0
                  ) +
         labs(title = paste0("el_stop_id: ", b),
              subtitle = "ACF (lags of 7): el_rides") +
         theme_minimal() +
         NULL
       }
     )



optimal_lag_setting <-
  six_stations_all_lags %>% 
  map(~ func_tidy_acf(.x,
                      value = el_rides,
                      lags = 0:max_lag
                      ) %>% 
        filter(lag != 0) %>% 
        top_n(n = 4, wt = acf) %>% 
        arrange(desc(acf)) %>% 
        # filter(acf == max(acf)
        #        ) %>% 
        pull(lag)
      )

optimal_lag_setting


rm(max_lag, optimal_lag_setting, func_tidy_acf)

```
  
    
  The optimal lags are slightly different for each `el_stop_id`, but adding lags of 7, 14, 21, and 28 covers almost all of the top for lags for each `el_stop_id` value.
```{r}

el_rides_lags <-
  six_stations_all_lags %>% 
  map(~ mutate(.x,
               el_rides_l07 = dplyr::lag(el_rides, n = 7),
               el_rides_l14 = dplyr::lag(el_rides, n = 14),
               el_rides_l21 = dplyr::lag(el_rides, n = 21),
               el_rides_l28 = dplyr::lag(el_rides, n = 28),
               )
      )

message("el_rides_lags")
str(el_rides_lags$`41140`)
# View(el_rides_lags$`41140`)


rm(six_stations_all_lags)

```
  
    
  Similarly, here I'll take rolling averages of `el_rides` with windows of 7, 14, 21, and 28 days.
```{r}

el_rides_ma_values <-
  el_rides_lags %>% 
  map(~ mutate(.x,
               el_rides_ma07 =
                 rollapply(data = el_rides,
                           width = 7,
                           mean,
                           na.rm = TRUE,
                           align = "right",
                           partial = FALSE,
                           fill = NA
                           ),
               el_rides_ma14 =
                 rollapply(data = el_rides,
                           width = 14,
                           mean,
                           na.rm = TRUE,
                           align = "right",
                           partial = FALSE,
                           fill = NA
                           ),
               el_rides_ma21 =
                 rollapply(data = el_rides,
                           width = 21,
                           mean,
                           na.rm = TRUE,
                           align = "right",
                           partial = FALSE,
                           fill = NA
                           ),
               el_rides_ma28 =
                 rollapply(data = el_rides,
                           width = 28,
                           mean,
                           na.rm = TRUE,
                           align = "right",
                           partial = FALSE,
                           fill = NA
                           )
               )
      )

message("el_rides_ma_values")
str(el_rides_ma_values$`41140`)
# View(el_rides_ma_values$`41140`)


rm(el_rides_lags)

```
  
    
  As the lags and windows can only start computing values with the 29th value, here I will simply remove the first 28 rows of data (rather than imputing the lag/mov avg values).
```{r}

remove_first_28na_rows <-
  pmap(.l = list(a = el_rides_ma_values),
       .f = function(a) {
         dat = a[29:nrow(a), ]
         
         return(dat)
         }
       )


message("remove_first_28na_rows")
str(remove_first_28na_rows$`41140`)
# View(remove_first_28na_rows$`41140`)


remove_first_28na_rows %>% 
  map(~ #select(.x,
        #        -matches("holiday")
        #        ) %>% 
        # miss_var_summary()
        miss_var_summary(.x)
      )


rm(el_rides_ma_values)

```


## Modeling

### Model Prep

#### Add An Indicator for Train-Validation-Test
  
  I'll be testing models that use the entire dataset, and also models that are specific to each `el_stop_id` (i.e., a model for each El entry point). Therefore, I'll create train-test splits (at 70% of the data) for each group.  
    
  First, I'll create a function to to create the proper value depending on the situation (i.e., creating the split for the entire dataset, or for each value of `el_stop_id` as a single entity).
```{r}

func_train_test_indicator <-
  function(data, date_var, train_val_pct, train_pct, new_col_suffix) {
    # get needed variables
    date_var_enquo = enquo(date_var)
    
    date_min = data %>% 
      select(!!date_var_enquo) %>% 
      distinct() %>% 
      top_n(n = -1, wt = !!date_var_enquo) %>% 
      pull(!!date_var_enquo)
    
    date_max = data %>% 
      select(!!date_var_enquo) %>% 
      distinct() %>% 
      top_n(n = 1, wt = !!date_var_enquo) %>% 
      pull(!!date_var_enquo)
    
    # train and validation split
    days_trainval = round(train_val_pct * (date_max - date_min))
    split_date_trainval = date_min + days_trainval

    # train split
    days_train = round(train_pct * (split_date_trainval - date_min))
    split_date_train = date_min + days_train

    # new dataset
    new_data =
      data %>%
      mutate(new_var =
               case_when(el_date < split_date_train ~ "01_train",
                         between(x = el_date,
                                 lower = split_date_train,
                                 upper = split_date_trainval,
                                 incbounds = TRUE
                                 ) ~ "02_validation",
                         TRUE ~ "03_test"
                         )
             )

    colnames(new_data)[length(new_data)] = paste0("data_use_", new_col_suffix)
    
    return(new_data)
  }


# test <-
#   func_train_test_indicator(data = bind_rows(six_stations_all_lags, .id = "id"),
#                             date_var = el_date,
#                             train_val_pct = 0.8,
#                             train_pct = 0.7,
#                             new_col_suffix = "all"
#                             )
# 
# colnames(bind_rows(six_stations_all_lags, .id = "id")
#          ) %>% tail(5)
# colnames(test) %>% tail(6)
# str(test)
# test %>% 
#   group_by(data_use_all) %>% 
#   summarise(min_el_date = min(el_date),
#             max_el_date = max(el_date)
#             )

# test_group <-
#   remove_first_28na_rows %>% 
#   map(~ func_train_test_indicator(data = .x,
#                                   date_var = el_date,
#                                   train_val_pct = 0.8,
#                                   train_pct = 0.7,
#                                   new_col_suffix = "el_stop_id"
#                                   )
#       ) 
# %>% 
#   bind_rows()

# str(test_group$`41140`)

# View(test_group$`41140` %>% 
#        select(el_date, el_stop_id, data_use_all, data_use_el_stop_id) #%>% 
#        # head(5000)
#      )

# View(
#   test_group %>% 
#   group_by(el_stop_id, data_use_all) %>% 
#   summarise(ed_min_a = min(el_date),
#             ed_max_a = max(el_date)
#             ) %>% 
#     left_join(y = test_group %>% 
#                 group_by(el_stop_id, data_use_el_stop_id) %>% 
#                 summarise(ed_min_s = min(el_date),
#                           ed_max_s = max(el_date)
#                           ),
#               by = c("el_stop_id" = "el_stop_id",
#                      "data_use_all" = "data_use_el_stop_id"
#                      )
#               ) %>% 
#     mutate(match_min = if_else(ed_min_a == ed_min_s, TRUE, FALSE),
#            match_max = if_else(ed_max_a == ed_max_s, TRUE, FALSE)
#            ) %>% 
#     filter(match_min == FALSE | match_max == FALSE)
# )


# View(test_group %>% 
#        select(el_stop_id, el_date, el_rides, data_use_all, data_use_el_stop_id) %>%
#        filter(el_stop_id %in% c(41690, 41700)
#               )
#      )


# View(
# test_group %>% 
#   count(el_stop_id, data_use_all) %>% 
#   rename(n_a = n) %>% 
#   left_join(y = test_group %>% 
#               count(el_stop_id, data_use_el_stop_id) %>% 
#               rename(n_es = n),
#             by = c("el_stop_id" = "el_stop_id", 
#                    "data_use_all" = "data_use_el_stop_id"
#                    )
#             ) %>% 
#   mutate(match = if_else(n_a == n_es, TRUE, FALSE)) %>% 
#   filter(match == FALSE)
# )
# 
# test_group %>% 
#   count(data_use_all)
# 
# test_group %>% 
#   count(data_use_el_stop_id)

```
  
    
  Here I run the function for each value of `el_stop_id`.
```{r}

add_trn_val_test <-
  remove_first_28na_rows %>% 
  map(~ func_train_test_indicator(data = .x,
                                  date_var = el_date,
                                  train_val_pct = 0.8,
                                  train_pct = 0.7,
                                  new_col_suffix = "el_stop_id"
                                  ) %>% 
        mutate(wday.lbl = factor(as.character(wday.lbl)
                                 )
               )
      ) 


## Save the data to the proper folder
saveRDS(add_trn_val_test,
        paste0(wd,
               "/Data/Interim/",
               "add_trn_val_test.Rds"
               )
        )


str(add_trn_val_test$`41140`)

add_trn_val_test %>% 
  map(~ group_by(.x,
                 data_use_el_stop_id
                 ) %>% 
        summarise(date_min = min(el_date),
                  date_max = max(el_date)
                  )
      )


rm(remove_first_28na_rows, func_train_test_indicator)

```



  **To Delete (below)**
  
```{r}

# add_trn_val_test <-
#   readRDS(paste0(wd,
#                  "/Data/Interim/",
#                  "add_trn_val_test.Rds"
#                  )
#           )
# 
# 
# str(add_trn_val_test$`41140`)

```

  **To Delete (above)**



```{r}


# # test_df_41140 <-
#   add_trn_val_test$`41140` %>% 
#   # filter(el_stop_id == 41140) %>% 
#   select(#-id,
#          -holiday_comment,
#          -holiday_comment_l7,
#          # -data_use_all,
#          -day
#          ) %>% 
#   mutate(data_use_el_stop_id = factor(data_use_el_stop_id)
#          ) %>% 
#   select(-holiday) %>% 
#   mutate_if(is.factor, factor) %>% 
#   summary()
# 
# # test_df_41140$el_rides %>% auto.arima()
# 
# pmap(.l = list(a = remove_first_28na_rows),
#      .f = function(a) {
#        dat = a$el_rides
#        
#        aa = auto.arima(dat)
#        }
#      )

```


#### Handling Factor Variables  
  
  Turn factor variables into dummy variables. **NOTE** that this is done with all levels of the factor. That is, it does **NOT** use "full rank parameterization" to leave one level out (e.g., to be used as the linear model's intercept).
```{r}

# create the function to create dummy vars
func_one_hot_vars <-
  function(x) {
    data_s = x %>% 
      select(-holiday_comment,
             -holiday,
             -day,
             -holiday_comment_l7,
             -holiday_l7
             )
    
    formula =
      dummyVars(el_rides ~ .,
                data = data_s %>% 
                  select(-el_date,
                         -el_stop_id,
                         -data_use_el_stop_id
                         ),
                fullRank = FALSE
                )
    
    data_DV =
      data_s %>% 
      select(el_rides,
             el_date,
             el_stop_id,
             data_use_el_stop_id
             ) %>% 
      bind_cols(predict(object = formula,
                        newdata = data_s %>% 
                          select(-el_date,
                                 -el_stop_id,
                                 -data_use_el_stop_id
                                 )
                        ) %>% 
                  as.data.frame()
                )
    }


# run the function
DV_data <-
  add_trn_val_test %>% 
  map(~ func_one_hot_vars(.x)
      )

glimpse(DV_data$`41140`)


# rm(func_one_hot_vars)

```
 
 
#### Accuracy Metrics
   
  I'll try both `caret::randomForest` and `caret::xgboost` on the dataset USING dummy variables. But first, I need to create some custom accuracy metrics.
```{r}

func_custom_accuracy_metrics <-
  function(data, lev = NULL, model = NULL) {
    mae =
      function(actual, predicted) {
        mean(abs((actual - predicted)
                 ),
             na.rm = TRUE
             )
        }
    
    mape =
      function(actual, predicted) {
        mean(abs((actual - predicted) / actual * 100),
             na.rm = TRUE
             )
        }
    
    rmse =
      function(actual, predicted) {
        sqrt(mean((actual - predicted)^2,
                  na.rm = TRUE
                  )
             )
        }
    
    r2 =
      function(actual, predicted) {
        1 - (sum((actual - predicted)^2
                 ) / sum((actual - mean(actual)
                          )^2
                         )
             )
    }
    
    
    out = c(mae(data$obs,
                data$pred
                ),
            mape(data$obs,
                 data$pred
                 ),
            rmse(data$obs,
                 data$pred
                 ),
            r2(data$obs,
               data$pred
               )
            )

    
    names(out) = c("MAE", "MAPE", "RMSE", "R2")
    
    out
    }

```


#### Models  
  
  First, limit to just training data, and confirm the datasets to use are the same (except for the one-hot-encoding producing dummy variables).
```{r}

train_data <-
  add_trn_val_test %>% 
  map(~ filter(.x,
               data_use_el_stop_id == "01_train"
               )
      )

DV_train_data <-
  DV_data %>% 
  map(~ filter(.x,
               data_use_el_stop_id == "01_train"
               )
      )


message("train_data")
train_data %>% 
  map(~ dim(.x)
      )

message("DV_train_data")
DV_train_data %>% 
  map(~ dim(.x)
      )


message("train_data")
train_data$`41140` %>% glimpse()

message("DV_train_data")
DV_train_data$`41140` %>% glimpse()

```


  Now I reduce the number of variables by using `caret::nearZeroVar` and `caret::corr`. This is done individually as `caret` will not handle a variable of zero standard deviation.  
    
  First, I use `caret::nearZeroVar` to remove variables with "near zero variance.
```{r}

DV_nzv_list <-
  DV_train_data %>%
  map(~ preProcess(.x,
                   # method = c("nzv", "corr", "center", "scale", "medianImpute"),
                   method = "nzv"
                   )
      )

DV_nzv_predict <-
  map2(.x = DV_nzv_list,
       .y = DV_train_data,
       .f = function(a, b) {
         predict(a, b)
         }
       )


saveRDS(DV_nzv_predict,
        paste0(wd,
               "/Data/Interim/",
               "DV_nzv_predict.Rds"
               )
        )

# DV_nzv_predict <-
#   readRDS(paste0(wd,
#                  "/Data/Interim/",
#                  "DV_nzv_predict.Rds"
#                  )
#           )


message("before reduction")
DV_train_data %>%
  map(~ dim(.x)
      )

DV_train_data$`41140` %>%
  glimpse()


message("after near-zero variable reduction")
DV_nzv_predict %>%
  map(~ dim(.x)
      )

DV_nzv_predict$`41140` %>%
  glimpse()


rm(DV_nzv_list)

```  


  First, I use `caret::corr` to remove highly correlated variables.
```{r}

DV_corr_list <-
  DV_nzv_predict %>%
  map(~ preProcess(.x,
                   # method = c("nzv", "corr", "center", "scale", "medianImpute"),
                   method = "corr"
                   )
      )

DV_corr_predict <-
  map2(.x = DV_corr_list,
       .y = DV_nzv_predict,
       .f = function(a, b) {
         predict(a, b)
         }
       )


saveRDS(DV_corr_predict,
        paste0(wd,
               "/Data/Interim/",
               "DV_corr_predict.Rds"
               )
        )

# DV_corr_predict <-
#   readRDS(paste0(wd,
#                  "/Data/Interim/",
#                  "DV_corr_predict.Rds"
#                  )
#           )


message("before corr reduction")
DV_nzv_predict %>%
  map(~ dim(.x)
      )

DV_nzv_predict$`41140` %>%
  glimpse()


message("after corr variable reduction")
DV_corr_predict %>%
  map(~ dim(.x)
      )

DV_corr_predict$`41140` %>%
  glimpse()


rm(DV_corr_list)

```  
  
    
  Modeling parameters used in multiple models.
```{r}

# period_train <- (365 * 2) + 30 # 2 years + 28 days of data needed for LSTM Keras modeling
# period_test <- (365 * 1) + 30 # test on 1 year * 28 days of data (even though we just predict 14 days out) needed for LSTM Keras modeling
# skip_span <- 7 * 2 # gives 14 evenly distributed  splits


period_train <- round((365 * 1.5),
                      digits = 0
                      ) + 30 # 2 years + 28 days of data needed for LSTM Keras modeling

period_test <- round((365 * 0.5),
                     digits = 0
                     ) + 30 # test on 1 year * 28 days of data (even though we just predict 14 days out) needed for LSTM Keras modeling

skip_span <- 8 # gives 13 evenly distributed  splits

```


### Random Forest  
  
 Create one model with preprocessing that removes highly correlated variables, and one model that does not.
```{r}

tot_cores <- detectCores()
cl <- makeCluster(tot_cores - 1)
registerDoParallel(cl)


start <- proc.time()
DV_Fit.Rf.corr_yes <-
  # DV_train_data[1:2] %>%
  DV_corr_predict %>%
  map(.f = function(a) {
    fitControl =
      trainControl(method = "timeslice",
                   # initialWindow = ceiling(nrow(a) * (1/3)
                   #                         ),
                   initialWindow = period_train,
                   # horizon = 7,
                   horizon = period_test,
                   fixedWindow = TRUE,
                   skip = skip_span,
                   # skip = 85,
                   summaryFunction = func_custom_accuracy_metrics
                   # preProcOptions = list(cutoff = 0.7)
                   )

    # grid_rf =
    #   expand.grid(mtry = seq(100, 500, 100)
    #               )

    set.seed(123456789)

    output =
      train(el_rides ~ .,
            data = a %>% 
              select(#-el_stop_id,
                     -data_use_el_stop_id
                     ),
            preProcess = c(#"nzv"
                           #"corr"
                           "center",
                           "scale",
                           "medianImpute"
                           ),
            na.action = na.pass,
            method = "rf",
            metric = "RMSE",
            maximize = FALSE,
            importance = TRUE,
            trControl = fitControl,
            # ntree = 1000,
            # tuneGrid = grid_rf,
            verbose = TRUE
            )

    return(output)
    }
    )

time.Rf.corr_yes <- proc.time() - start

message("DV_Fit.Rf.corr_yes")
DV_Fit.Rf.corr_yes


start <- proc.time()
DV_Fit.Rf.corr_no <-
  # DV_train_data %>%
  DV_nzv_predict %>%
  map(.f = function(a) {
    fitControl =
      trainControl(method = "timeslice",
                   # initialWindow = ceiling(nrow(a) * (1/3)
                   #                         ),
                   initialWindow = period_train,
                   # horizon = 7,
                   horizon = period_test,
                   fixedWindow = TRUE,
                   skip = skip_span,
                   # skip = 85,
                   summaryFunction = func_custom_accuracy_metrics
                   # preProcOptions = list(cutoff = 0.7)
                   )
    
    # grid_rf =
    #   expand.grid(mtry = seq(100, 500, 100)
    #               )
    
    set.seed(123456789)
    
    output =
      train(el_rides ~ .,
            data = a %>% 
              select(#-el_stop_id,
                     -data_use_el_stop_id
                     ),
            preProcess = c(#"nzv"
                           #"corr"
                           "center",
                           "scale",
                           "medianImpute"
                           ),
            na.action = na.pass,
            method = "rf",
            metric = "RMSE",
            maximize = FALSE,
            importance = TRUE,
            trControl = fitControl,
            # ntree = 1000,
            # tuneGrid = grid_rf,
            verbose = TRUE
            )
    
    return(output)
    }
    )

time.Rf.corr_no <- proc.time() - start

message("DV_Fit.Rf.corr_no")
DV_Fit.Rf.corr_no


stopCluster(cl)
rm(start, tot_cores, cl)

```
  
    
  Compare the results.
```{r}

# user  system elapsed 
#  61.039   5.166 527.512 
# ~ 9 min
message("time.Rf.corr_yes")
time.Rf.corr_yes

# user  system elapsed 
#  58.048   3.563 486.738
# ~ 8 min
message("time.Rf.corr_no")
time.Rf.corr_no


# Create a list of models
Models.Rf <-
  pmap(.l = list(a = DV_Fit.Rf.corr_yes,
                 b = DV_Fit.Rf.corr_no
                 ),
       .f = function(a, b) {
         l = list(Corr_No = a,
                  Corr_Yes = b
                  )
         
         return(l)
         }
       )


# Resample the models
Resample_Results.Rf <-
  Models.Rf %>% 
  map(~ resamples(.x)
      )


# Generate a summary
Resample_Results.Rf %>% 
  map(~ summary(.x)
      )

Resample_Results.Rf %>% 
  map(~ bwplot(.x)
      )

```
  
    
  After inspecting the results, we choose to keep the model that includes the correlation filter in the preprocessing stage - the results and runtimes were similar.
```{r}

rm(list = ls(pattern = "corr_yes"))


saveRDS(DV_Fit.Rf.corr_no,
        paste0(wd,
               "/Models/",
               "DV_Fit.Rf.corr_no.Rds"
               )
        )


saveRDS(time.Rf.corr_no,
        paste0(wd,
               "/Models/",
               "time.Rf.corr_no.Rds"
               )
        )


# DV_Fit.Rf.corr_no <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "DV_Fit.Rf.corr_no.Rds"
#                  )
#           )

# time.Rf.corr_no <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "time.Rf.corr_no.Rds"
#                  )
#           )

```
  
    
  Inspect varialbe importance.
```{r}

# Permutation improtance is used for the variable importance
# Based on discussion here:  http://parrt.cs.usfca.edu/doc/rf-importance/index.html
VI <- DV_Fit.Rf.corr_no %>% 
  map(~ varImp(.x,
               type = 1,
               scale = TRUE
               )
      )

VI


VI %>% 
  map(~ plot(.x, top = 20)
      )


rm(VI)

```


### Extreme Gradient Boosted Tree
  
 Create one model with preprocessing that removes highly correlated variables, and one model that does not.
```{r}

tot_cores <- detectCores()
cl <- makeCluster(tot_cores - 1)
registerDoParallel(cl)


start <- proc.time()
DV_Fit.Xgbtree.corr_yes <-
  # DV_train_data[1:2] %>%
  DV_corr_predict %>%
  map(.f = function(a) {
    fitControl =
      trainControl(method = "timeslice",
                   # initialWindow = ceiling(nrow(a) * (1/3)
                   #                         ),
                   initialWindow = period_train,
                   # horizon = 7,
                   horizon = period_test,
                   fixedWindow = TRUE,
                   skip = skip_span,
                   # skip = 85,
                   summaryFunction = func_custom_accuracy_metrics
                   # preProcOptions = list(cutoff = 0.7)
                   )

    # grid_rf =
    #   expand.grid(mtry = seq(100, 500, 100)
    #               )

    set.seed(123456789)

    output =
      train(el_rides ~ .,
            data = a %>% 
              select(#-el_stop_id,
                     -data_use_el_stop_id
                     ),
            preProcess = c(#"nzv"
                           #"corr"
                           "center",
                           "scale",
                           "medianImpute"
                           ),
            na.action = na.pass,
            method = "xgbTree",
            metric = "RMSE",
            maximize = FALSE,
            importance = TRUE,
            trControl = fitControl,
            # ntree = 1000,
            # tuneGrid = grid_rf,
            verbose = TRUE
            )

    return(output)
    }
    )

time.Xgbtree.corr_yes <- proc.time() - start

# message("DV_Fit.Xgbtree.corr_yes")
# DV_Fit.Xgbtree.corr_yes



start <- proc.time()
DV_Fit.Xgbtree.corr_no <-
  # DV_train_data %>%
  DV_nzv_predict %>%
  map(.f = function(a) {
    fitControl =
      trainControl(method = "timeslice",
                   # initialWindow = ceiling(nrow(a) * (1/3)
                   #                         ),
                   initialWindow = period_train,
                   # horizon = 7,
                   horizon = period_test,
                   fixedWindow = TRUE,
                   skip = skip_span,
                   # skip = 85,
                   summaryFunction = func_custom_accuracy_metrics
                   # preProcOptions = list(cutoff = 0.7)
                   )
    
    # grid_rf =
    #   expand.grid(mtry = seq(100, 500, 100)
    #               )
    
    set.seed(123456789)
    
    output =
      train(el_rides ~ .,
            data = a %>% 
              select(#-el_stop_id,
                     -data_use_el_stop_id
                     ),
            preProcess = c(#"nzv"
                           #"corr"
                           "center",
                           "scale",
                           "medianImpute"
                           ),
            na.action = na.pass,
            method = "xgbTree",
            metric = "RMSE",
            maximize = FALSE,
            importance = TRUE,
            trControl = fitControl,
            # ntree = 1000,
            # tuneGrid = grid_rf,
            verbose = TRUE
            )
    
    return(output)
    }
    )

time.Xgbtree.corr_no <- proc.time() - start

# message("DV_Fit.Xgbtree.corr_no")
# DV_Fit.Xgbtree.corr_no


stopCluster(cl)
rm(start, tot_cores, cl)

```

    
  Compare the results.
```{r}

# user  system elapsed 
#  10.888   2.333 179.411
# ~ 3 min 
message("time.Xgbtree.corr_yes")
time.Xgbtree.corr_yes

# user  system elapsed 
#  10.377   2.360 201.333
# ~ 3 min
message("time.Xgbtree.corr_no")
time.Xgbtree.corr_no


# Create a list of models
Models.Xgbtree <-
  pmap(.l = list(a = DV_Fit.Xgbtree.corr_yes,
                 b = DV_Fit.Xgbtree.corr_no
                 ),
       .f = function(a, b) {
         l = list(Corr_No = a,
                  Corr_Yes = b
                  )
         
         return(l)
         }
       )


# Resample the models
Resample_Results.Xgbtree <-
  Models.Xgbtree %>% 
  map(~ resamples(.x)
      )


# Generate a summary
Resample_Results.Xgbtree %>% 
  map(~ summary(.x)
      )

Resample_Results.Xgbtree %>% 
  map(~ bwplot(.x)
      )

```
  
    
  After inspecting the results, we choose to keep the model that does NOT include the correlation filter in the preprocessing stage - the results were similar, and the run time was about half as long.
```{r}

rm(list = ls(pattern = "corr_no"))


saveRDS(DV_Fit.Xgbtree.corr_yes,
        paste0(wd,
               "/Models/",
               "DV_Fit.Xgbtree.corr_yes.Rds"
               )
        )


saveRDS(time.Xgbtree.corr_yes,
        paste0(wd,
               "/Models/",
               "time.Xgbtree.corr_yes.Rds"
               )
        )

# DV_Fit.Xgbtree.corr_yes <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "DV_Fit.Xgbtree.corr_yes.Rds"
#                  )
#           )

# time.Xgbtree.corr_yes <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "time.Xgbtree.corr_yes.Rds"
#                  )
#           )

```
  
    
  Inspect varialbe importance.
```{r}

# Permutation improtance is used for the variable importance
# Based on discussion here:  http://parrt.cs.usfca.edu/doc/rf-importance/index.html
VI <- DV_Fit.Xgbtree.corr_yes %>% 
  map(~ varImp(.x,
               type = 1,
               scale = TRUE
               )
      )

VI


VI %>% 
  map(~ plot(.x, top = 20)
      )


rm(VI)

```


### Rolling Resamples  
  
  This is based on the example shown [here](https://topepo.github.io/rsample/articles/Applications/Time_Series.html), and is needed for the time-series-ish models, which are not currently part of the `caret` modeling process.
    
  Create the rolling-origin resamples to be used for measuring forecast accuracy. Rolling samples will use 2 years of data do predict the next 14 days.  
```{r}

roll_rs <-
  # DV_train_data %>% 
  train_data %>% 
  map(~ rolling_origin(.x,
                       initial = period_train,
                       assess = period_test,
                       cumulative = FALSE,
                       skip = skip_span
                       )
      )


message("DV_train_data")
DV_train_data %>% 
  map(~ nrow(.x)
      )

message("roll_rs")
roll_rs %>% 
  map(~ nrow(.x)
      )


names(roll_rs$`40600`)
roll_rs$`40600`$splits[[1]]


# rm(period_train, period_test, skip_span)

```
  
    
  For plotting, let’s index each split by the first day of the assessment set.
```{r}

get_date <-
  function(x)
    min(assessment(x)$el_date
        )


roll_rs <-
  pmap(.l = list(a = roll_rs),
       .f = function(a) {
         data = a
         
         splits = data$splits %>% 
           map(get_date)
         
         data$start_date = do.call("c", splits)
         
         return(data)
         }
       )

names(roll_rs$`40600`)
length(roll_rs$`40600`$start_date)
head(roll_rs$`40600`$start_date, 20)


rm(get_date)

```



### Model Setup (`forecast::auto.arima` and `prophet::prophet`)
      
  Here, we use `forecast::auto.arima` to produce create an arima model. We also try `prophet::prophet` to create a model based on trend, seasonality, and holidays, and that is a bit like a general additive model. More info can be found [here](https://facebook.github.io/prophet/) and [here](https://peerj.com/preprints/3190/).  
    
  First, I create the function for the basic ARIMA model.
```{r}

fit_model_arima <-
  function(x, ...) {
    # suggested by Matt Dancho:
    data = x %>% 
      # base = x %>% 
      analysis() %>% 
      # Since the first day changes over resamples, adjust it based on the first date value in the data frame
      tk_ts(select = el_rides,
            start = .$el_date[[1]] %>% lubridate::year(),
            # freq = 365.25,
            # freq = 365,
            freq = 7,
            silent = TRUE
            )
    
    fit = auto.arima(data, ...)
    
    # fourier_trans = fourier(base, K = 7)
    # 
    # res = auto.arima(base,
    #                  xreg = fourier_trans
    #                  )
    return(fit)
    }


# View(holiday_dates)
# View(analysis(roll_rs$`ttb.gov/foia/frl.shtml`$splits[[1]]) %>% 
#        left_join(holiday_dates,
#                  by = c("date" = "ds")
#                  ) %>% 
#        mutate(holiday_binary = if_else(!is.na(holiday),
#                                        1,
#                                        0
#                                        )
#               )
#      )
# arima with fourier transformation and holidays in xreg

```
  
    
  Next I create the function to run an ARIMA model with external regressors (external regressors include fourier transformations, and other regressors identified by using the random forest and xgboost models used above).
```{r}

fit_model_arima_xreg <-
  function(x, ...) {
    # suggested by Matt Dancho:
    data = 
      x %>%
      # models$`40600`$splits[[1]] %>% 
      # base = x %>% 
      analysis() %>% 
      mutate(holiday_binary = if_else(holiday == FALSE,
                                      0,
                                      1
                                      )
             )
    
    # weekly frequency
    ts_7 =
      data %>% 
      tk_ts(select = el_rides,
            start = .$el_date[[1]] %>% lubridate::year(),
            freq = 7,
            silent = TRUE
            )
    
    # yearly frequency
    ts_365 =
      data %>% 
      tk_ts(select = el_rides,
            start = .$el_date[[1]] %>% lubridate::year(),
            # freq = 365.25,
            freq = 365,
            silent = TRUE
            )
    
    # use a fourier transformation to capture daily seasonality and choose K programatically
    bestfit = list(aicc = Inf)
    for(K in seq(7)
        ) {
      n = nrow(assessment(x)
               )
      # n = nrow(assessment(models$`40600`$splits[[1]])
      #          )
      
      ts_365_fourier = fourier(ts_365,
                               K = K
                               )
      
      ts_365_fourier_future = fourier(ts_365,
                                      K = K,
                                      h = n
                                      )
      
      fit = auto.arima(ts_7,
                       xreg = cbind(ts_365_fourier,
                                    data$holiday_binary,
                                    data$year,
                                    data$half,
                                    data$quarter,
                                    data$month,
                                    data$mweek,
                                    data$wday.lbl,
                                    data$el_rides_l07,
                                    data$el_rides_l14,
                                    data$el_rides_l21,
                                    data$el_rides_l28,
                                    data$el_rides_ma07,
                                    data$el_rides_ma14,
                                    data$el_rides_ma21,
                                    data$el_rides_ma28#,
                                    # data$divvy_pt5mi_stn_cnt,
                                    # data$divvy_mindist_miles,
                                    # data$tmin_bands,
                                    # data$tmax_bands
                                    
                                    # data$pageview_sum_lag21,
                                    # data$pageview_sum_lag28,
                                    # data$bounces_sum_lag21,
                                    # data$bounces_sum_lag28,
                                    # data$timeOnPage_sum21,
                                    # data$timeOnPage_sum28,
                                    # data$avgTimeOnPage_avg_lag21,
                                    # data$avgTimeOnPage_avg_lag28
                                    ),
                       # seasonal = FALSE
                       )
      
      if(fit[["aicc"]] < bestfit[["aicc"]]) {
        bestfit = fit
        bestK = K
        bestts_365_fourier_future = ts_365_fourier_future
      }
      
      return(list(best_fit = bestfit,
                  best_k = bestK,
                  best_ts_365_fourier_future = bestts_365_fourier_future
                  )
             )
    }
    
    return()
    }

```
  
    
  Now I create the function to run the basic prophet model (based just on `el_rides`.
```{r}

# prophet
fit_model_prophet <-
  function(x, ...) {
    # suggested by Matt Dancho:
    x %>% 
      analysis() %>% 
      # Since the first day changes over resamples, adjust it based on the first date value in the data frame
      # tk_ts(select = pageview_sum,
      #       start = .$date[[1]] %>% lubridate::year(),
      #       freq = 365.25,
      #       silent = TRUE
      #       ) %>% 
      select(el_date,
             el_rides
             ) %>% 
      rename(ds = el_date,
             y = el_rides
             ) %>% 
      prophet(...)
  }

```


### Run Models

#### Arima-Based Model  
  **Need to review this:**  [http://www.business-science.io/code-tools/2018/04/08/introducing-anomalize.html](http://www.business-science.io/code-tools/2018/04/08/introducing-anomalize.html) for anomaly detection.
    
  Can also use any other modeling methods (e.g., prophet), and then use the outlier detection method twitter used (Generalized ESD) on the remainder. **See:**  [https://www.rdocumentation.org/packages/EnvStats/versions/2.3.0/topics/rosnerTest](https://www.rdocumentation.org/packages/EnvStats/versions/2.3.0/topics/rosnerTest) for the algorithm used outside of `AnomalyDetection::AnomalyDetectionTs`.  
    
  Here I run the basic `forecast::auto.arima` model.
```{r}

# user   system  elapsed 
# 1001.467   21.414 1036.904
# ~ 17 min
message("arima")
start <- proc.time()
models <-
  pmap(.l = list(a = roll_rs),
       .f = function(a) {
         
         splits_a = a$splits %>%
           map(fit_model_arima)
         
         a$arima = splits_a
         
         return(a)
         }
       )

time.arima <- proc.time() - start
time.arima

```
  
    
  Here I run the `forecast::auto.arima` model with external regressors.
```{r}
 
# user  system elapsed 
# 626.467  26.937 673.471
# ~ 11 min
message("arima_xreg")
start <- proc.time()
models <-
  pmap(.l = list(a = models),
       .f = function(a) {
         
         splits_a_xreg = a$splits %>% 
           map(fit_model_arima_xreg)
         
         a$arima_xreg = splits_a_xreg
         
         return(a)
         }
       )

time.arima_xreg <- proc.time() - start
time.arima_xreg

```
  
    
  Here I run the basic `prophet::prophet` model
```{r}

# user  system elapsed 
#  12.007   1.344  13.901
message("prophet")
start <- proc.time()
models <-
  pmap(.l = list(a = models),
       .f = function(a) {
         
         splits_p = a$splits %>%
           map(~ fit_model_prophet(.x,
                                   
                                   )
               )
         
         a$prophet = splits_p
         
         return(a)
         }
       )

time.prophet <- proc.time() - start
time.prophet

```
  
    
  Here I run the `prophet::prophet` model that includes regressors for holidays.
```{r}

# create the holiday_dates
holiday_dates <-
  add_trn_val_test %>% 
  map(~ select(.x,
               holiday_name,
               el_date
               ) %>% 
        filter(holiday_name != "--Not_Holiday--") %>% 
        rename(holiday = holiday_name,
               ds = el_date
               )
      )

holiday_dates <- bind_rows(holiday_dates) %>% 
  select(holiday,
         ds
         ) %>% 
  distinct() %>% 
  arrange(ds)


# user  system elapsed 
#  35.327   3.401  47.577
message("prophet_hol")
start <- proc.time()
models <-
  pmap(.l = list(a = models),
       .f = function(a) {
         
         splits_p_hol = a$splits %>%
           map(~ fit_model_prophet(.x,
                                   holidays = holiday_dates
                                   )
               )
           
         a$prophet_hol <- splits_p_hol
         
         return(a)
         }
       )

time.prophet_hol <- proc.time() - start
time.prophet_hol


# rm(holiday_dates)

```



```{r}

run_times <- list(arima = as.list(time.arima),
                  arima_xreg = as.list(time.arima_xreg),
                  prophet = as.list(time.prophet),
                  prophet_hol = as.list(time.prophet_hol)
                  )

run_times


# saving is done to avoid having to run the models again
saveRDS(models,
        paste0(wd,
               "/Models/",
               "models.Rds"
               )
        )

# models <-
#   readRDS(paste0(wd,
#                  "/Data_Processed/",
#                  "models.Rds"
#                  )
#           )


# names(models)
# names(models$`ttb.gov/foia/frl.shtml`)
# names(models$`ttb.gov/foia/frl.shtml`$arima_xreg[[10]])
# nrow(models$`ttb.gov/foia/frl.shtml`$arima_xreg[[10]]$best_ts_365_fourier_future)
# models$`ttb.gov/foia/frl.shtml`$arima_xreg[[10]]$best_fit
# 
# str(models$`ttb.gov/foia/frl.shtml`$splits[[3]])



# rm(roll_rs)
# rm(time.arima, time.arima_xreg, time.prophet, time.prophet_hol)

```
  
    
  Create the `Prophet` forecasts and plots - these will be used for measuring model accuracy (below). Here I creat the future prophet dataset.
```{r}

# Dataframe for future dates
prophet.future <-
  pmap(.l = list(a = models),
       .f = function(a) {
         splits_p_future =
           a$prophet %>% 
           map(make_future_dataframe,
               periods = 365
               )
         
         splits_p_hol_future =
           a$prophet_hol %>% 
           map(make_future_dataframe,
               periods = 365
               )
         
         a$prophet.future <- splits_p_future
         a$prophet_hol.future <- splits_p_hol_future
         
         return(a)
         }
       )

length(prophet.future$`40600`$prophet.future)
length(prophet.future$`40600`$prophet_hol.future)
# prophet.future$`40600`$prophet.future[[10]]
# prophet.future$`40600`$prophet_hol.future[[10]]

```  
  
    
  And now I can create the prohpet forecasts. First, the basic prophet forecast.
```{r}

# user  system elapsed 
# 220.704  21.132 242.524
# ~ 4 min
start <- proc.time()
prophet.forecast <-
  pmap(.l = list(a = prophet.future),
       .f = function(a) {
         splits_p_m =
           a$prophet
         
         splits_p_future =
           a$prophet.future
         
         splits_p_forecast =
           pmap(.l = list(b = splits_p_m,
                          c = splits_p_future
                          ),
                .f = function(b, c) {
                  predict(b, c) %>% 
                    # if_else is needed to prevent negative predictions
                    mutate(yhat_zero_floor = if_else(yhat < 0,
                                                     0,
                                                     yhat
                                                     )
                           )
                  }
                )
         
         a$prophet.forecast = splits_p_forecast
         
         return(a)
         }
       )

time.prophet.forecast <- proc.time() - start
time.prophet.forecast

```
  
    
  And now the prophet forecast including holidays.
```{r}

# user  system elapsed 
# 792.629  78.698 881.271
# ~ 15 min
start <- proc.time()
prophet.forecast <-
  pmap(.l = list(a = prophet.forecast),
       .f = function(a) {
         splits_p_hol_m =
           a$prophet_hol
         
         splits_p_hol_future =
           a$prophet_hol.future
         
         splits_p_hol_forecast =
           pmap(.l = list(b = splits_p_hol_m,
                          c = splits_p_hol_future
                          ),
                .f = function(b, c) {
                  predict(b, c) %>% 
                    # if_else is needed to prevent negative predictions
                    mutate(yhat_zero_floor = if_else(yhat < 0,
                                                     0,
                                                     yhat
                                                     )
                           )
                  }
                )
         
         a$prophet_hol.forecast = splits_p_hol_forecast
         
         return(a)
         }
       )

time.prophet_hol.forecast <- proc.time() - start
time.prophet_hol.forecast

```
  
    
  Now I simply update the `run_times` dataset with the relevant prophet info, and the relvant info from the random forest and xgboost models.
```{r}


run_times[5:6] <-
  list(prophet.forecast = as.list(time.prophet.forecast),
       prophet_hol.forecast = as.list(time.prophet_hol.forecast)
       )

names(run_times)[5:6] <- c("prophet.forecast", "prophet_hol.forecast")

run_times[7:8] <-
  list(rf_corr_no = as.list(time.Rf.corr_no),
       xgbtree_corr_yes = as.list(time.Xgbtree.corr_yes)
       )
  
names(run_times)[7:8] <- c("rf_corr_no", "xgbtree_corr_yes")


str(run_times)


# saving is done to avoid having to run the forecasts again
saveRDS(run_times,
        paste0(wd,
               "/Models/",
               "run_times.Rds"
               )
        )

# run_times <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "run_times.Rds"
#                  )
#           )


rm(time.Rf.corr_no, time.Xgbtree.corr_yes)

```
  
    
  And now I can create the prophet plots.
```{r}

prophet.plots <-
  pmap(.l = list(a = prophet.forecast),
       .f = function(a) {
         splits_p_m =
           a$prophet
         
         splits_p_hol_m =
           a$prophet_hol
         
         splits_p_forecast =
           a$prophet.forecast
         
         splits_p_hol_forecast =
           a$prophet_hol.forecast
         
         
         splits_p_plots =
           pmap(.l = list(b = splits_p_m,
                          c = splits_p_forecast
                          ),
                .f = function(b, c) {
                  plot(b, c)
                  }
                )
         
         splits_p_hol_plots =
           pmap(.l = list(b = splits_p_hol_m,
                          c = splits_p_hol_forecast
                          ),
                .f = function(b, c) {
                  plot(b, c)
                  }
                )
         
         
         a$prophet.plots <- splits_p_plots
         a$prophet_hol.plots <- splits_p_hol_plots
         
         return(a)
         }
       )

# saving is done to avoid having to run the forecasts again
saveRDS(prophet.plots,
        paste0(wd,
               "/Models/",
               "prophet.plots.Rds"
               )
        )

# prophet.plots <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "prophet.plots.Rds"
#                  )
#           )


names(prophet.plots)
names(prophet.plots$`40600`)
length(prophet.plots$`40600`$prophet.plots)
length(prophet.plots$`40600`$prophet_hol.plots)

message("prophet")
pmap(.l = list(a = prophet.plots
               ),
     .f = function(a, b) {
       dat = a
       
       dat$prophet.plots[[10]]
       }
     )

message("prophet_hol")
pmap(.l = list(a = prophet.plots
               ),
     .f = function(a, b) {
       dat = a
       
       dat$prophet_hol.plots[[10]]
       
       }
     )

# prophet.plots$`40600`$prophet.plots[[10]]
# prophet.plots$`40600`$prophet_hol.plots[[10]]
# prophet.plots$`40600`$prophet[[10]]
# prophet.plots$`40600`$prophet_hol[[10]]


# rm(models, prophet.future, prophet.forecast, time.prophet.forecast, time.prophet_hol.forecast, start)

```


## Modeling With H2O


### All Variables  
  
  First, create one-hot variables from the `add_trn_val_test` dataset.
```{r}

# One-Hot
h2o_one_hot <-
  add_trn_val_test %>% 
  map(~ func_one_hot_vars(.x))


# create train-val-test for h2o
h2o_train_tbl <-
  h2o_one_hot %>% 
  map(~ (filter(.x,
                data_use_el_stop_id == "01_train"
                )
         )
      )

h2o_valid_tbl <-
  h2o_one_hot %>% 
  map(~ (filter(.x,
                data_use_el_stop_id == "02_validation"
                )
         )
      )

h2o_test_tbl <-
  h2o_one_hot %>% 
  map(~ (filter(.x,
                data_use_el_stop_id == "03_test"
                )
         )
      )


# Fire up h2o
h2o.init()


# Create the Interpolation & Extrapolation Datasets
h2o_data <-
  pmap(.l = list(a = prophet.plots),
       .f = function(a) {
         dat_interp = a$splits %>% 
           map(~ analysis(.x) %>% 
                 func_one_hot_vars() %>% 
                 as.h2o()
               )
         
         dat_extrap = a$splits %>% 
           map(~ assessment(.x) %>% 
                 func_one_hot_vars() %>% 
                 as.h2o()
               )
         
         a$interp_data = dat_interp
         a$extrap_data = dat_extrap
         
         return(a)
         }
       )

# h2o_data$`40600`$interp_data[[1]] %>% as.data.frame() %>% dim()
rm(prophet.plots)


# Convert to H2OFrame objects
h2o_train <- h2o_train_tbl %>% map(~ as.h2o(.x))
h2o_valid <- h2o_valid_tbl %>% map(~ as.h2o(.x))
h2o_test  <- h2o_test_tbl %>% map(~ as.h2o(.x))


# Set names for h2o
y <- "el_rides"
x <- h2o_train %>% 
  map(~ setdiff(names(.x),
                y
                )
      )


# user  system elapsed 
#   9.730   5.819 215.764
# ~ 4 min
start <- proc.time()

h2o_automl_models <-
  pmap(.l = list(a = x,
                 b = h2o_train,
                 c = h2o_valid,
                 d = h2o_test
                 ),
       .f = function(a, b, c, d) {
         h2o.automl(
           x = a,
           y = y,
           training_frame = b,
           validation_frame = c,
           leaderboard_frame = d,
           nfolds = 13,
           # max_runtime_secs = 300,
           max_runtime_secs = 30,
           stopping_metric = "deviance"
           )
         }
       )

h2o.time <- proc.time() - start
rm(start)

h2o.time


saveRDS(h2o.time,
        paste0(wd,
               "/Models/",
               "h2o.time.Rds"
               )
        )

# h2o.time <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "h2o.time.Rds"
#                  )
#           )

# print(automl_models_h2o$`40600`@leaderboard)


# Extract leader model
h2o_automl_leader <-
  pmap(.l = list(a = h2o_automl_models),
       .f = function(a) {
         ldr = a@leader
         
         return(ldr)
         }
       )


# save the leader
saveRDS(h2o_automl_leader,
        paste0(wd,
               "/Models/",
               "h2o_automl_leader.Rds"
               )
        )

# h2o_automl_leader <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "h2o_automl_leader.Rds"
#                  )
#           )


# get predictions on test set
h2o_pred <-
  pmap(.l = list(a = h2o_automl_leader,
                 b = h2o_test
                 ),
       .f = function(a, b) {
         res = h2o.predict(a, newdata = b)
         
         return(res)
         }
       )


# get predictions on interp & extrap
h2o_forecasts <-
  pmap(.l = list(a = h2o_data,
                 b = h2o_automl_leader
                 ),
       .f = function(a, b) {
         int_pred =
           pmap(.l = list(c = a$interp_data),
                .f = function(c) {
                  pred = h2o.predict(b, newdata = c) %>% 
                    as.data.frame()
                  
                  return(pred)
                  }
                )

         ext_pred =
           pmap(.l = list(d = a$extrap_data),
                .f = function(d) {
                  pred = h2o.predict(b, newdata = d) %>% 
                    as.data.frame()
                  
                  return(pred)
                  }
                )
         
         
         int_data =
           pmap(.l = list(e = a$interp_data),
                .f = function(e) {
                  df = e %>% as.data.frame()
                  
                  return(df)
                  }
                )
         
         ext_data =
           pmap(.l = list(f = a$extrap_data),
                .f = function(f) {
                  df = f %>% as.data.frame()
                  
                  return(df)
                  }
                )
         
         
         int_fcast =
           pmap(.l = list(g = int_data,
                          h = int_pred
                          ),
                .f = function(g, h) {
                  res = g %>% 
                    select(el_date, el_rides) %>% 
                    bind_cols(h) %>% 
                    rename(actual = el_rides,
                           yhat = predict
                           ) %>% 
                    mutate(sqrd_error = (actual - yhat)^2
                           )
                  }
                )
         
         ext_fcast =
           pmap(.l = list(i = ext_data,
                          j = ext_pred
                          ),
                .f = function(i, j) {
                  res = i %>% 
                    select(el_date, el_rides) %>% 
                    bind_cols(j) %>% 
                    rename(actual = el_rides,
                           yhat = predict
                           ) %>% 
                    mutate(sqrd_error = (actual - yhat)^2
                           )
                  }
                )
         
         
         a$h2o.interp.forecast = int_fcast
         a$h2o.extrap.forecast = ext_fcast
         
         return(a)
         }
       )

# add_el_stop_id$`40600`$h2o.extrap.forecast[[1]] %>% str()
rm(h2o_data)


# accuracy
h2o_accuracy_stats <-
  pmap(.l = list(a = h2o_forecasts),
       .f = function(a) {
         rmse_interp =
           a$h2o.interp.forecast %>% 
           map_dbl(~ sqrt(mean(.x$sqrd_error)
                          )
               )
         
         rmse_extrap =
           a$h2o.extrap.forecast %>% 
           map_dbl(~ sqrt(mean(.x$sqrd_error)
                          )
               )
         
         a$h2o.interpolation = rmse_interp
         a$h2o.extrapolation = rmse_extrap
         
         return(a)
         }
       )


# save the dataset 
saveRDS(h2o_accuracy_stats,
        paste0(wd,
               "/Models/",
               "h2o_accuracy_stats.Rds"
               )
        )


# h2o_accuracy_stats <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "h2o_accuracy_stats.Rds"
#                  )
#           )


rm(h2o_forecasts)


# Summary stats of accuracy  
h2o_accuracy_stats %>% 
  map(~ summary(.x$h2o.interpolation)
      )

h2o_accuracy_stats %>% 
  map(~ summary(.x$h2o.extrapolation)
      )



# measure performance on test dataset
h2o_perf <-
  pmap(.l = list(a = h2o_automl_leader,
                 b = h2o_test),
       .f = function(a, b) {
         per = h2o.performance(a, newdata = b)
         
         return(per)
         }
       )

h2o_perf


# save the leader performance
saveRDS(h2o_perf,
        paste0(wd,
               "/Models/",
               "h2o_perf.Rds"
               )
        )


# h2o_perf <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "h2o_perf.Rds"
#                  )
#           )


# h2o.removeAll()

```


### Limited Variables

```{r}

# select specific columns to use
col_names <-
  DV_corr_predict %>% 
  map(~ colnames(.x))


select_cols <-
  pmap(.l = list(a = h2o_one_hot,
                 b = col_names
                 ),
       .f = function(a, b) {
         res = a[, colnames(a) %in% b]
         
         return(res)
         }
       )


rm(col_names)


# create train-val-test for h2o
h2o.limvars_train_tbl <-
  select_cols %>% 
  map(~ (filter(.x,
                data_use_el_stop_id == "01_train"
                )
         )
      )

h2o.limvars_valid_tbl <-
  select_cols %>% 
  map(~ (filter(.x,
                data_use_el_stop_id == "02_validation"
                )
         )
      )

h2o.limvars_test_tbl <-
  select_cols %>% 
  map(~ (filter(.x,
                data_use_el_stop_id == "03_test"
                )
         )
      )


rm(select_cols)


# Fire up h2o
# h2o.init()


# Convert to H2OFrame objects
h2o.limvars_train <- h2o.limvars_train_tbl %>% map(~ as.h2o(.x))
h2o.limvars_valid <- h2o.limvars_valid_tbl %>% map(~ as.h2o(.x))
h2o.limvars_test  <- h2o.limvars_test_tbl %>% map(~ as.h2o(.x))


# Set names for h2o
y <- "el_rides"
x <- h2o.limvars_train %>% 
  map(~ setdiff(names(.x),
                y
                )
      )


# user  system elapsed 
#   7.985   3.463 208.442 
# ~ 4 min
start <- proc.time()

h2o.limvars_automl_models <-
  pmap(.l = list(a = x,
                 b = h2o.limvars_train,
                 c = h2o.limvars_valid,
                 d = h2o.limvars_test
                 ),
       .f = function(a, b, c, d) {
         h2o.automl(
           x = a,
           y = y,
           training_frame = b,
           validation_frame = c,
           leaderboard_frame = d,
           nfolds = 13,
           # max_runtime_secs = 300,
           max_runtime_secs = 30,
           stopping_metric = "deviance"
           )
         }
       )

h2o.limvars.time <- proc.time() - start
rm(start)

h2o.limvars.time


saveRDS(h2o.limvars.time,
        paste0(wd,
               "/Models/",
               "h2o.limvars.time.Rds"
               )
        )

# print(h2o.limvars_automl_models$`40600`@leaderboard)


# Extract leader model
h2o.limvars_automl_leader <-
  pmap(.l = list(a = h2o.limvars_automl_models),
       .f = function(a) {
         ldr = a@leader
         
         return(ldr)
         }
       )


# save the leader
saveRDS(h2o.limvars_automl_leader,
        paste0(wd,
               "/Models/",
               "h2o.limvars_automl_leader.Rds"
               )
        )

# h2o.limvars_automl_leader <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "h2o.limvars_automl_leader.Rds"
#                  )
#           )


# get predictions on test set
h2o.limvars_pred <-
  pmap(.l = list(a = h2o.limvars_automl_leader,
                 b = h2o.limvars_test
                 ),
       .f = function(a, b) {
         res = h2o.predict(a, newdata = b)
         
         return(res)
         }
       )


# get predictions on interp & extrap
h2o.limvars_forecasts <-
  pmap(.l = list(a = h2o_accuracy_stats,
                 b = h2o.limvars_automl_leader
                 ),
       .f = function(a, b) {
         int_pred =
           pmap(.l = list(c = a$interp_data),
                .f = function(c) {
                  pred = h2o.predict(b, newdata = c) %>% 
                    as.data.frame()
                  
                  return(pred)
                  }
                )
         
         ext_pred =
           pmap(.l = list(d = a$extrap_data),
                .f = function(d) {
                  pred = h2o.predict(b, newdata = d) %>% 
                    as.data.frame()
                  
                  return(pred)
                  }
                )
         
         
         int_data =
           pmap(.l = list(e = a$interp_data),
                .f = function(e) {
                  df = e %>% as.data.frame()
                  
                  return(df)
                  }
                )
         
         ext_data =
           pmap(.l = list(f = a$extrap_data),
                .f = function(f) {
                  df = f %>% as.data.frame()
                  
                  return(df)
                  }
                )
         
         
         int_fcast =
           pmap(.l = list(g = int_data,
                          h = int_pred
                          ),
                .f = function(g, h) {
                  res = g %>% 
                    select(el_date, el_rides) %>% 
                    bind_cols(h) %>% 
                    rename(actual = el_rides,
                           yhat = predict
                           ) %>% 
                    mutate(sqrd_error = (actual - yhat)^2
                           )
                  }
                )
         
         ext_fcast =
           pmap(.l = list(i = ext_data,
                          j = ext_pred
                          ),
                .f = function(i, j) {
                  res = i %>% 
                    select(el_date, el_rides) %>% 
                    bind_cols(j) %>% 
                    rename(actual = el_rides,
                           yhat = predict
                           ) %>% 
                    mutate(sqrd_error = (actual - yhat)^2
                           )
                  }
                )
         
         
         a$h2o.limvars.interp.forecast = int_fcast
         a$h2o.limvars.extrap.forecast = ext_fcast
         
         return(a)
         }
       )


rm(h2o_accuracy_stats)


h2o.limvars_accuracy_stats <-
  pmap(.l = list(a = h2o.limvars_forecasts),
       .f = function(a) {
         rmse_interp =
           a$h2o.limvars.interp.forecast %>% 
           map_dbl(~ sqrt(mean(.x$sqrd_error)
                          )
               )
         
         rmse_extrap =
           a$h2o.limvars.extrap.forecast %>% 
           map_dbl(~ sqrt(mean(.x$sqrd_error)
                          )
               )
         
         a$h2o.limvars.interpolation = rmse_interp
         a$h2o.limvars.extrapolation = rmse_extrap
         
         return(a)
         }
       )


# save the dataset 
saveRDS(h2o.limvars_accuracy_stats,
        paste0(wd,
               "/Models/",
               "h2o.limvars_accuracy_stats.Rds"
               )
        )


# h2o.limvars_accuracy_stats <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "h2o.limvars_accuracy_stats.Rds"
#                  )
#           )


rm(h2o.limvars_forecasts)


# Summary stats of accuracy  
h2o.limvars_accuracy_stats %>% 
  map(~ summary(.x$h2o.limvars.interpolation)
      )

h2o.limvars_accuracy_stats %>% 
  map(~ summary(.x$h2o.limvars.extrapolation)
      )



# measure performance on test dataset
h2o.limvars_perf <-
  pmap(.l = list(a = h2o.limvars_automl_leader,
                 b = h2o.limvars_test),
       .f = function(a, b) {
         per = h2o.performance(a, newdata = b)
         
         return(per)
         }
       )

h2o.limvars_perf


# save the leader performance
saveRDS(h2o.limvars_perf,
        paste0(wd,
               "/Models/",
               "h2o.limvars_perf.Rds"
               )
        )


# h2o.limvars_perf <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "h2o.limvars_perf.Rds"
#                  )
#           )


# h2o.removeAll()

```


## Accuracy Measurements  
    
  Now we can check some summary (RMSE) statistics. **NOTE: these are summary statistics based on the data used to build the models.**  
    
### Interpolation - ARIMA Model
```{r}

accuracy_interp_arima <-
  pmap(.l = list(a = h2o.limvars_accuracy_stats),
       .f = function(a) {
           
         interp_a =
           map_dbl(a$arima,
                   function(x) 
                     sw_glance(x)[["RMSE"]]
                   )
         
         interp_a_xreg =
           map_dbl(a$arima_xreg,
                   function(x) {
                     y = x$best_fit %>% 
                       sw_glance()
                     
                     y[["RMSE"]]
                     }
                   )
         
         a$arima.interpolation = interp_a
         a$arima_xreg.interpolation = interp_a_xreg
         
         return(a)
         }
       )


names(accuracy_interp_arima$`40600`)
accuracy_interp_arima$`40600`$arima.interpolation
accuracy_interp_arima$`40600`$arima_xreg.interpolation

message("arima.interpolation")
accuracy_interp_arima %>% 
  map(~ summary(.x$arima.interpolation)
      )

message("arima_xreg.interpolation")
accuracy_interp_arima %>% 
  map(~ summary(.x$arima_xreg.interpolation)
      )


rm(h2o.limvars_accuracy_stats)

```


### Interpolation - Prophet Model
```{r}

get_interp_prophet <- function(split, pf) {
  # Get analysis data
  pred_dat =
    analysis(split) %>% 
    select(el_date,
           el_rides
         ) %>% 
    left_join(select(pf,
                     ds,
                     yhat,
                     yhat_zero_floor
                     ) %>% 
                mutate_at(vars(ds), as.Date),
              by = c("el_date" = "ds")
            ) %>% 
    # mutate(pct_error = (el_rides - yhat_zero_floor) / el_rides * 100
    #        ) %>% 
    mutate(sqrd_error = (el_rides - yhat_zero_floor)^2
           ) %>% 
    arrange(el_date)
  
  # mean(abs(pred_dat$pct_error)
  #      )
  sqrt(mean(pred_dat$sqrd_error,
            na.rm = TRUE
            )
       )
  }    
  

accuracy_interp_prophet <-
  pmap(.l = list(a = accuracy_interp_arima),
       .f = function(a) {
         interp =
           map2_dbl(a$splits,
                    a$prophet.forecast,
                    get_interp_prophet
                    )
         
         interp_hol =
           map2_dbl(a$splits,
                    a$prophet_hol.forecast,
                    get_interp_prophet
                    )
         
         a$prophet.interpolation = interp
         
         a$prophet_hol.interpolation = interp_hol
         
         return(a)
         }
       )


names(accuracy_interp_prophet$`40600`)
accuracy_interp_prophet$`40600`$prophet.interpolation
accuracy_interp_prophet$`40600`$prophet_hol.interpolation

message("prophet.interpolation")
accuracy_interp_prophet %>% 
  map(~ summary(.x$prophet.interpolation)
      )

message("prophet_hol.interpolation")
accuracy_interp_prophet %>% 
  map(~ summary(.x$prophet_hol.interpolation)
      )


# saving is done to avoid having to run the models again
saveRDS(accuracy_interp_prophet,
        paste0(wd,
               "/Models/",
               "accuracy_interp_prophet.Rds"
               )
        )

# accuracy_interp_prophet <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "accuracy_interp_prophet.Rds"
#                  )
#           )


rm(accuracy_interp_arima)

```



### Forecast & Interpolation - Random Forest Model  
  
  First, add the info of the Random Forest and the Xgboost model to the "base" list file.
```{r}

# test <-
#   accuracy_interp_prophet
# 
# x <- DV_Fit.Rf.corr_yes$`ttb.gov/foia/frl.shtml`
# as.tibble(x)
# 
# names(test$`ttb.gov/foia/frl.shtml`)
# test$`ttb.gov/foia/frl.shtml`$rf <- as.list(x)
# test$`ttb.gov/foia/frl.shtml`$rf[[1]] <- x
# 
# test_2 <-
#   pmap(.l = list(a = test,
#                  b = DV_Fit.Rf.corr_yes,
#                  c = DV_Fit.Xgbtree.corr_yes
#                  ),
#        .f = function(a, b, c) {
#          a$rf = b
#          a$xgb = c
#          
#          return(a)
#          }
#        )
# 
# names(test_2)

```
  
    
  Get the forecasted data for the random forest and the xgboost tree.  
    
  First, I create a function to get the error percentages.
```{r}

get_pct_error <-
  function(base, mod) {
    data = bind_cols(base,
                     mod
                     ) %>% 
      select(el_date,
             el_rides,
             yhat
             ) %>%
      rename(y = el_rides
             ) %>%
      # if_else is needed to prevent negative predictions
      mutate(yhat_zero_floor = if_else(yhat < 0,
                                       0,
                                       yhat
                                       ),
             # pct_error = (y - yhat_zero_floor) / y * 100,
             sqrd_error = (y - yhat_zero_floor)^2
             ) %>%
      arrange(el_date)
    
    return(data)
    }

```
  
    
  Now I get the forecasts of all ML models as one list.
```{r}

rf_xgb_forecasts_combined <-
  pmap(.l = list(a = accuracy_interp_prophet,
                 b = DV_Fit.Rf.corr_no,
                 c = DV_Fit.Xgbtree.corr_yes
                 ),
       .f = function(a, b, c) {
         splits = a$splits
         rf = b
         xgb = c
         
         fcast =
           pmap(.l = list(d = splits),
                .f = function(d) {
                  # prep the data
                  analysis_assessment =
                    analysis(d) %>% 
                    mutate(type = "interpolation") %>% 
                    bind_rows(assessment(d) %>% 
                                mutate(type = "extrapolation")
                              )
                  
                  one_hot =
                    analysis_assessment %>% 
                    func_one_hot_vars()
                  
                  one_hot_interp =
                    one_hot %>% 
                    filter(typeinterpolation == 1) %>% 
                    select(-typeinterpolation,
                           -typeextrapolation
                           )
                  
                  one_hot_extrap = 
                    one_hot %>% 
                    filter(typeextrapolation == 1) %>% 
                    select(-typeinterpolation,
                           -typeextrapolation
                           )
                  
                  complete_interp =
                    one_hot_interp[complete.cases(one_hot_interp), ]
                  
                  complete_extrap =
                    one_hot_extrap[complete.cases(one_hot_extrap), ]
                  
                  
                  # predictions
                  pred_rf.interp =
                    predict(rf,
                            newdata = complete_interp
                            ) %>% 
                    as.data.frame()
                  
                  names(pred_rf.interp) <- "yhat"
                  
                  pred_rf.extrap =
                    predict(rf,
                            newdata = complete_extrap
                            ) %>% 
                    as.data.frame()
                  
                  names(pred_rf.extrap) <- "yhat"
                  
                  
                  pred_xgb.interp =
                    predict(xgb,
                            newdata = complete_interp
                            ) %>% 
                    as.data.frame()
                  
                  names(pred_xgb.interp) <- "yhat"
                  
                  pred_xgb.extrap =
                    predict(xgb,
                            newdata = complete_extrap
                            ) %>% 
                    as.data.frame()
                  
                  names(pred_xgb.extrap) <- "yhat"
                  
                  
                  # add percent errors
                  full_pred.rf.interp =
                    get_pct_error(base = complete_interp,
                                  mod = pred_rf.interp
                                  )
                  
                  full_pred.rf.extrap =
                    get_pct_error(base = complete_extrap,
                                  mod = pred_rf.extrap
                                  )
                  
                  
                  full_pred.xgb.interp =
                    get_pct_error(base = complete_interp,
                                  mod = pred_xgb.interp
                                  )
                  
                  full_pred.xgb.extrap =
                    get_pct_error(base = complete_extrap,
                                  mod = pred_xgb.extrap
                                  )
                  
                  
                  # result as a list
                  full_pred_list =
                    list(pred.rf.interp = full_pred.rf.interp,
                         pred.rf.extrap = full_pred.rf.extrap,
                         pred.xgb.interp = full_pred.xgb.interp,
                         pred.xgb.extrap = full_pred.xgb.extrap
                         )
                  
                  # return value
                  return(full_pred_list)
                  }
                )
         
         a$ml_forecasts_all = fcast
         
         return(a)
         }
       )

names(rf_xgb_forecasts_combined$`40600`)

```
  
    
  Now I separate the forecats into their own dataframes.
```{r}

rf_xgb_forecasts_individualized <-
  pmap(.l = list(a = rf_xgb_forecasts_combined),
       .f = function(a) {
         fc = a$ml_forecasts_all

         # limit results to the relevant model
         result.rf.interp =
           pmap(.l = list(b = fc),
              .f = function(b) {
                b$pred.rf.interp
                }
              )
         
         result.rf.extrap =
           pmap(.l = list(b = fc),
              .f = function(b) {
                b$pred.rf.extrap
                }
              )
         
         result.xgb.interp =
           pmap(.l = list(b = fc),
              .f = function(b) {
                b$pred.xgb.interp
                }
              )
         
         result.xgb.extrap =
           pmap(.l = list(b = fc),
              .f = function(b) {
                b$pred.xgb.extrap
                }
              )
         
         a$rf.forecast_interp <- result.rf.interp
         a$rf.forecast_extrap <- result.rf.extrap
         a$xgb.forecast_interp <- result.xgb.interp
         a$xgb.forecast_extrap <- result.xgb.extrap
         
         # remove no longer needed list of all forecasts
         a$ml_forecasts_all <- NULL
         
         return(a)
         }
       )


names(rf_xgb_forecasts_individualized$`40600`)
# rf_xgb_forecasts_individualized$`40600`$rf.forecast_interp[[13]]
# rf_xgb_forecasts_individualized$`40600`$xgb.forecast_extrap[[13]]


rm(accuracy_interp_prophet, rf_xgb_forecasts_combined)

```
  
    
  Now I calculate the interpolation and extrapolation accuracy stats.
```{r}

# function to get the RMSE value
get_interp_ML <- function(ML.fcast) {
  pred_dat = ML.fcast
  
  # mean(abs(pred_dat$pct_error)
  #      )
  sqrt(mean(pred_dat$sqrd_error,
            na.rm = TRUE
            )
       )
  }


accuracy_interp_ML <-
  pmap(.l = list(a = rf_xgb_forecasts_individualized),
       .f = function(a) {
         interp.rf =
           map_dbl(a$rf.forecast_interp,
                   get_interp_ML
                   )
         
         extrap.rf =
           map_dbl(a$rf.forecast_extrap,
                   get_interp_ML
                   )
         
         interp.xgb =
           map_dbl(a$xgb.forecast_interp,
                   get_interp_ML
                   )
         
         extrap.xgb =
           map_dbl(a$xgb.forecast_extrap,
                   get_interp_ML
                   )
         
         a$rf.interpolation <- interp.rf
         a$rf.extrapolation <- extrap.rf
         a$xgb.interpolation <- interp.xgb
         a$xgb.extrapolation <- extrap.xgb
         
         return(a)
         }
       )


names(accuracy_interp_ML$`40600`)
# accuracy_interp_ML$`40600`$rf_forecast[[13]]
accuracy_interp_ML$`40600`$rf.interpolation
accuracy_interp_ML$`40600`$rf.extrapolation


message("rf.interpolation")
accuracy_interp_ML %>% 
  map(~ summary(.x$rf.interpolation)
      )

message("rf.extrapolation")
accuracy_interp_ML %>% 
  map(~ summary(.x$rf.extrapolation)
      )

message("xgb.interpolation")
accuracy_interp_ML %>% 
  map(~ summary(.x$xgb.interpolation)
      )

message("xgb.extrapolation")
accuracy_interp_ML %>% 
  map(~ summary(.x$xgb.extrapolation)
      )


# saving is done to avoid having to run the models again
saveRDS(accuracy_interp_ML,
        paste0(wd,
               "/Models/",
               "accuracy_interp_ML.Rds"
               )
        )

# accuracy_interp_ML <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "accuracy_interp_ML.Rds"
#                  )
#           )


rm(rf_xgb_forecasts_individualized)

```
  
    
  Now we can check some summary (RMSE) statistics. **NOTE: these are summary statistics based on the data NOT used to build the models.**  
    
### Extrapolation - ARIMA Model  
  
  Function to get the ARIMA predictions.
```{r}

get_pred_arima <- function(split, mod) {
  n <- nrow(assessment(split)
            )
  # Get assessment data
  pred_dat <-
    assessment(split) %>%
    mutate(pred = as.vector(forecast(mod, h = n)$mean),
           # if_else is used to put a floor of 0 (zero) for the prediction
           pred_zero_floor = if_else(pred < 0,
                                     0,
                                     pred
                                     ),
           # pct_error = (el_rides - pred_zero_floor) / el_rides * 100,
           sqrd_error = (el_rides - pred_zero_floor)^2
           )
  
  return(pred_dat)
  }

```
  
    
  Function to calculate the extrapolation stats
```{r}

get_extrap_arima <- function(arima.fcast) {
  pred_dat = arima.fcast
  
  # mean(abs(pred_dat$pct_error)
  #      )
  sqrt(mean(pred_dat$sqrd_error,
            na.rm = TRUE
            )
       )
  }

```
  
    
  Run the the forecast and extrapolation functions.
```{r}

accuracy_extrap_arima <-
  pmap(.l = list(a = accuracy_interp_ML),
       .f = function(a) {
         pred =
           map2(a$splits,
                a$arima,
                get_pred_arima
               )
         
         a$arima.forecast <- pred
         
         return(a)
         }
       )


accuracy_extrap_arima <-
  pmap(.l = list(a = accuracy_extrap_arima),
       .f = function(a) {
         extrap <-
           map_dbl(a$arima.forecast,
                   get_extrap_arima
                   )
         
         a$arima.extrapolation <- extrap
         
         return(a)
         }
       )

names(accuracy_extrap_arima$`40600`)
# accuracy_extrap_arima$`40600`$arima.forecast[[13]]
accuracy_extrap_arima$`40600`$arima.extrapolation

accuracy_extrap_arima %>% 
  map(~ summary(.x$arima.extrapolation)
      )


rm(accuracy_interp_ML)

```


### Extrapolation - ARIMA Model with XREG  
  
  Function to get the forecasts from the ARIMA XREG models. 
```{r}

get_pred_arima_xreg <-
  function(split, mod) {
    n = nrow(assessment(split)
             )
    
    data_og =
      split %>% 
      assessment %>% 
      mutate(holiday_binary = if_else(holiday == FALSE,
                                      0,
                                      1
                                      )
             )
    
    data_pred =
      data_og %>% 
      mutate(pred = as.vector(forecast(mod$best_fit,
                                       xreg = cbind(mod$best_ts_365_fourier_future,
                                                    data_og$holiday_binary,
                                                    data_og$year,
                                                    data_og$half,
                                                    data_og$quarter,
                                                    data_og$month,
                                                    data_og$mweek,
                                                    data_og$wday.lbl,
                                                    data_og$el_rides_l07,
                                                    data_og$el_rides_l14,
                                                    data_og$el_rides_l21,
                                                    data_og$el_rides_l28,
                                                    data_og$el_rides_ma07,
                                                    data_og$el_rides_ma14,
                                                    data_og$el_rides_ma21,
                                                    data_og$el_rides_ma28#,
                                                    # data_og$divvy_pt5mi_stn_cnt,
                                                    # data_og$divvy_mindist_miles,
                                                    # data_og$tmin_bands,
                                                    # data_og$tmax_bands
                                                    
                                                    # data_og$pageview_sum_lag21,
                                                    # data_og$pageview_sum_lag28,
                                                    # data_og$bounces_sum_lag21,
                                                    # data_og$bounces_sum_lag28,
                                                    # data_og$timeOnPage_sum21,
                                                    # data_og$timeOnPage_sum28,
                                                    # data_og$avgTimeOnPage_avg_lag21,
                                                    # data_og$avgTimeOnPage_avg_lag28
                                    ),
                                       h = n
                                       )$mean
                              ),
             # if_else is used to put a floor of 0 (zero) for the prediction
             pred_zero_floor = if_else(pred < 0,
                                       0,
                                       pred
                                       ),
             # pct_error = (el_rides - pred_zero_floor) / el_rides * 100
             sqrd_error = (el_rides - pred_zero_floor)^2
             )
    }

```
  
    
  Function to calculate the extrapolation stats.
```{r}

get_extrap_arima_xreg <-
  function(arima.fcast) {
    pred_dat = arima.fcast
    
    # mean(abs(pred_dat$pct_error)
    #      )
    sqrt(mean(pred_dat$sqrd_error,
              na.rm = TRUE
              )
         )
    }

```
  
    
  Run the forecast and extrapolation functions.
```{r}

accuracy_extrap_arima_xreg <-
  pmap(.l = list(a = accuracy_extrap_arima),
       .f = function(a) {
         pred <-
           map2(a$splits,
                a$arima_xreg,
                get_pred_arima_xreg
               )
         
         a$arima_xreg.forecast <- pred
         
         return(a)
         }
       )


accuracy_extrap_arima_xreg <-
  pmap(.l = list(a = accuracy_extrap_arima_xreg),
       .f = function(a) {
         extrap <-
           map_dbl(a$arima_xreg.forecast,
                   get_extrap_arima
                   )
         
         a$arima_xreg.extrapolation <- extrap
         
         return(a)
         }
       )

names(accuracy_extrap_arima_xreg$`40600`)
# accuracy_extrap_arima_xreg$`40600`$arima_xreg.forecast[[13]]
accuracy_extrap_arima_xreg$`40600`$arima_xreg.extrapolation

accuracy_extrap_arima_xreg %>% 
  map(~ summary(.x$arima_xreg.extrapolation)
      )


rm(accuracy_extrap_arima)


```


### Extrapolation - Prophet Model  
  
  Function to get the extrapolation stats.
```{r}

get_extrap_prophet <- function(split, pf) {
  # Get assessment data
  pred_dat <-
    assessment(split) %>% 
    select(el_date,
           el_rides
         ) %>% 
    left_join(select(pf,
                     ds,
                     yhat,
                     yhat_zero_floor
                     ) %>% 
                mutate_at(vars(ds), as.Date),
              by = c("el_date" = "ds")
            ) %>% 
    mutate(#pct_error = (el_rides - yhat_zero_floor) / el_rides * 100,
           sqrd_error = (el_rides - yhat_zero_floor)^2 
           ) %>% 
    arrange(el_date)
  
  # mean(abs(pred_dat$pct_error)
  #      )
  sqrt(mean(pred_dat$sqrd_error,
            na.rm = TRUE
            )
       )
  }    
  
```
  
    
  Run the extrapolation stats function.
```{r}

# accuracy_extrap_arima_xreg$`ttb.gov/foia/frl.shtml`$splits[[1]] %>% 
#   assessment() %>% 
#   select(date,
#          pageview_sum
#          ) %>% 
#   left_join(select(accuracy_extrap_arima_xreg$`ttb.gov/foia/frl.shtml`$prophet.forecast[[1]],
#                    ds,
#                    yhat,
#                    yhat_zero_floor
#                    ) %>% 
#               mutate_at(vars(ds), as.Date),
#               by = c("date" = "ds")
#             ) %>% 
#   mutate(#pct_error = (pageview_sum - yhat_zero_floor) / pageview_sum * 100,
#            sqrd_error = (pageview_sum - yhat_zero_floor)^2 
#            ) %>% 
#     arrange(date)

accuracy_extrap_prophet <-
  pmap(.l = list(a = accuracy_extrap_arima_xreg),
       .f = function(a) {
         extrap <-
           map2_dbl(a$splits,
                    a$prophet.forecast,
                    get_extrap_prophet
                    )
         
         extrap_hol <-
           map2_dbl(a$splits,
                    a$prophet_hol.forecast,
                    get_extrap_prophet
                    )
         
         a$prophet.extrapolation <- extrap
         
         a$prophet_hol.extrapolation <- extrap_hol
         
         return(a)
         }
       )


# saving is done to avoid having to run the models again
saveRDS(accuracy_extrap_prophet,
        paste0(wd,
               "/Models/",
               "accuracy_extrap_prophet.Rds"
               )
        )

# accuracy_extrap_prophet <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "accuracy_extrap_prophet.Rds"
#                  )
#           )

names(accuracy_extrap_prophet$`40600`)
accuracy_extrap_prophet$`40600`$prophet.extrapolation
accuracy_extrap_prophet$`40600`$prophet_hol.extrapolation

message("prophet")
accuracy_extrap_prophet %>% 
  map(~ summary(.x$prophet.extrapolation)
      )

message("prophet_hol")
accuracy_extrap_prophet %>% 
  map(~ summary(.x$prophet_hol.extrapolation)
      )


rm(accuracy_extrap_arima_xreg)

```


## Visualize the Accuracy Measures

 Add an identifier for the relevant `el_stop_id` value.
```{r}
  
add_el_stop_id <-
  pmap(.l = list(a = accuracy_extrap_prophet,
                 c = names(accuracy_extrap_prophet)
                 ),
       .f = function(a, c) {
         a$el_stop_id <- c
         
         return(a)
       }
       )

# saving is done to avoid having to run the forecasts again
saveRDS(add_el_stop_id,
        paste0(wd,
               "/Models/",
               "add_el_stop_id.Rds"
               )
        )

# add_el_stop_id <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "add_el_stop_id.Rds"
#                  )
#           )


names(add_el_stop_id$`40600`)


rm(accuracy_extrap_prophet)

```
  
    
  Now I simply update the `run_times` dataset with the relevant h2o info.
```{r}


run_times[9:10] <-
  list(h2o = as.list(h2o.time),
       h2o.limvars = as.list(h2o.limvars.time)
       )

names(run_times)[9:10] <- c("h2o", "h2o.limvars")


str(run_times)


# saving is done to avoid having to run the forecasts again
saveRDS(run_times,
        paste0(wd,
               "/Models/",
               "run_times.Rds"
               )
        )

# run_times <-
#   readRDS(paste0(wd,
#                  "/Models/",
#                  "run_times.Rds"
#                  )
#           )

```
  
   
    
  Plot interpolation and extrapolation over time, by model.
```{r}

plot_interp_extrap <-
  add_el_stop_id %>% 
  map(~ select(.x,
               arima.interpolation,
               arima.extrapolation,
               arima_xreg.interpolation,
               arima_xreg.extrapolation,
               prophet.interpolation,
               prophet.extrapolation,
               prophet_hol.interpolation,
               prophet_hol.extrapolation,
               rf.interpolation,
               rf.extrapolation,
               xgb.interpolation,
               xgb.extrapolation,
               h2o.interpolation,
               h2o.extrapolation,
               h2o.limvars.interpolation,
               h2o.limvars.extrapolation,
               start_date,
               el_stop_id
               ) %>%
        as.data.frame %>% 
        rename(h2o_limvars.interpolation = h2o.limvars.interpolation,
               h2o_limvars.extrapolation = h2o.limvars.extrapolation
               ) %>% 
        gather(error, RMSE, -start_date, -el_stop_id) %>% 
        separate(error,
                 c("model", "type"),
                 sep = "\\.",
                 remove = FALSE
                 ) %>% 
        ggplot(aes(x = start_date,
                   y = RMSE,
                   # col = error
                   color = model
                   )
               ) +
        facet_wrap(~ type,
                   scales = "free"
                   ) +
        geom_point() +
        geom_line() +
        ggtitle(label = .$el_stop_id#,
                # subtitle = "ARIMA-based model"
                ) +
        theme_bw() +
        theme(legend.position = "bottom")
      )

plot_interp_extrap

```
  
    
  For each `el_stop_id`, plot the median RMSE for each model. First I munge the data.
```{r}

median_RMSE_by_model <-
  pmap(.l = list(a = add_el_stop_id),
       .f = function(a) {
         med.arima = median(a$arima.extrapolation)
         med.arima_xreg = median(a$arima_xreg.extrapolation)
         med.prophet = median(a$prophet.extrapolation)
         med.prophet_hol = median(a$prophet_hol.extrapolation)
         med.rf = median(a$rf.extrapolation)
         med.xgb = median(a$xgb.extrapolation)
         med.h2o = median(a$h2o.extrapolation)
         med.h2o_limvars = median(a$h2o.limvars.extrapolation)
         
         med_extrap_all_models =
           data.frame(el_stop_id = unique(a$el_stop_id),
                      arima = med.arima,
                      arima_xreg = med.arima_xreg,
                      prophet = med.prophet,
                      prophet_hol = med.prophet_hol,
                      rf = med.rf,
                      xgb = med.xgb,
                      h2o = med.h2o,
                      h2o_limvars = med.h2o_limvars
                      )
         
         return(med_extrap_all_models)
         }
       ) %>% 
  bind_rows()


# View(median_RMSE_by_model)

```
  
    
  Now I create the plot itself.
```{r}

median_RMSE_by_model %>% 
  gather(key = "model",
         value = "RMSE_med",
         -el_stop_id
         ) %>% 
  ggplot(aes(x = model,
             y = RMSE_med,
             fill = model
             )
         ) +
  geom_col() +
  geom_text(aes(label = format(round(RMSE_med, 1),
                               1
                               )
                ),
            size = 3,
            hjust = 1
            # nudge_y = -50
            ) +
  facet_wrap(~ el_stop_id,
             ncol = 2,
             scales = "free"
             ) +
  # scale_y_continuous(limits = c(0, max(test$RMSE_med)),
  #                    breaks = seq(0, max(test$RMSE_med), 25)
  #                    ) +
  # expand_limits(y = range_act) +
  coord_flip() +
  theme_bw() +
  theme(legend.position = "none")


rm(median_RMSE_by_model)

```


## Visualize Actual vs Models
**NOTE: plotting is just for extrapolation data**  
  
  Prepare the data by first getting all actuals and forecasts.
```{r}

# function to munge the data
func_get_forecast_data <-
  function(el_stop_id) {
    data_assessment_list =
      map(.x = el_stop_id$splits,
          .f = function(a) {
            extrap =
              assessment(a) %>% 
              select(el_date,
                     el_stop_id,
                     el_rides
                     ) %>% 
              rename(actual = el_rides)
            
            return(extrap)
            }
          )
  # add_el_stop_id$`40600`$splits %>% assessment() %>% select(el_date, el_stop_id, el_rides) %>% rename(actual = el_rides) %>% nrow()
  data_arima_list =
    el_stop_id$arima.forecast %>% 
    map(~ select(.x,
                 el_date,
                 # full_url,
                 pred_zero_floor
                 ) %>% 
          # mutate(model_arima = "arima") %>%
          rename(yhat.arima = pred_zero_floor)
        )
 
  data_arima_xreg_list =
    el_stop_id$arima_xreg.forecast %>% 
    map(~ select(.x,
                 el_date,
                 # full_url,
                 pred_zero_floor
                 ) %>% 
          # mutate(model_arima = "arima") %>%
          rename(yhat.arima_xreg = pred_zero_floor)
        )
  
  data_prophet_list =
    el_stop_id$prophet.forecast %>% 
    map(~ select(.x,
                 ds,
                 yhat_zero_floor
                 ) %>% 
          mutate_at(vars(ds), as.Date) %>% 
          # mutate(model_prophet = "prophet") %>% 
          rename(yhat.prophet = yhat_zero_floor)
        )
  
  data_prophet_hol_list =
    el_stop_id$prophet_hol.forecast %>% 
    map(~ select(.x,
                 ds,
                 yhat_zero_floor
                 ) %>% 
          mutate_at(vars(ds), as.Date) %>% 
          # mutate(model_prophet = "prophet") %>% 
          rename(yhat.prophet_hol = yhat_zero_floor)
        )
  
  data_rf_list =
    el_stop_id$rf.forecast_extrap %>% 
    map(~ select(.x,
                 el_date,
                 yhat_zero_floor
                 ) %>% 
          rename(yhat.rf = yhat_zero_floor)
        )
  
  data_xgb_list =
    el_stop_id$xgb.forecast_extrap %>% 
    map(~ select(.x,
                 el_date,
                 yhat_zero_floor
                 ) %>% 
          rename(yhat.xgb = yhat_zero_floor)
        )
  # add_el_stop_id$`40600`$h2o.limvars.extrap.forecast[[1]] %>% 
  #   rename(yhat.h2o = yhat) %>%  
  #   mutate(el_date = as_date(el_date)
  #          ) %>% 
  #   str()
  data_h2o_list = 
    el_stop_id$h2o.extrap.forecast %>% 
    map(~ rename(.x,
                 yhat.h2o = yhat
                 ) %>% 
          # mutate(el_date = as_date(el_date)
          #        ) %>% 
          select(yhat.h2o)
        )
  
  data_h2o_limvars_list = 
    el_stop_id$h2o.limvars.extrap.forecast %>% 
    map(~ rename(.x,
                 yhat.h2o_limvars = yhat
                 ) %>% 
          # mutate(el_date = as_date(el_date)
          #        ) %>% 
          select(yhat.h2o_limvars)
        )
  
  full_data_list =
    pmap(.l = list(a = data_assessment_list,
                   b = data_arima_list,
                   c = data_arima_xreg_list,
                   d = data_prophet_list,
                   e = data_prophet_hol_list,
                   f = data_rf_list,
                   g = data_xgb_list, 
                   h = data_h2o_list,
                   i = data_h2o_limvars_list
                   ),
         .f = function(a, b, c, d, e, f, g, h, i) {
           a %>% 
             left_join(b,
                       by = "el_date"
                       ) %>% 
             left_join(c,
                       by = "el_date"
                       ) %>% 
             left_join(d,
                       by = c("el_date" = "ds")
                       ) %>% 
             left_join(e,
                       by = c("el_date" = "ds")
                       ) %>% 
             left_join(f,
                       by = "el_date"
                       ) %>% 
             left_join(g,
                       by = "el_date"
                       ) %>% 
             # left_join(h,
             #           by = "el_date"
             #           ) %>% 
             # left_join(i,
             #           by = "el_date"
             #           ) %>% 
             bind_cols(h) %>% 
             bind_cols(i)
           }
         )
  
  full_data = bind_rows(full_data_list)
  
  return(full_data)
  }


# run the function for each value of `el_stop_id`
forecast_data_list <-
  add_el_stop_id %>% 
  map(~ func_get_forecast_data(.x)
      )

```
  
    
  Now I munge the resulting data for plotting.
```{r}

# View(forecast_data_list$`40600`)
forecast_data_to_plot_list <-
  forecast_data_list %>% 
  map(~ gather(.x,
               key = "key",
               value = "el_rides",
               yhat.arima,
               yhat.arima_xreg,
               yhat.prophet,
               yhat.prophet_hol,
               yhat.rf,
               yhat.xgb,
               yhat.h2o,
               yhat.h2o_limvars,
               actual
               ) %>% 
        arrange(el_date,
                key
                )
      )

# View(forecast_data_to_plot_list[[1]])


rm(forecast_data_list)

```
  
    
  And here I create the plot itself.
```{r}

clrs <- c("actual" = "black",
          "yhat.arima" = "red",
          "yhat.arima_xreg" = "green",
          "yhat.prophet" = "blue",
          "yhat.prophet_hol" = "orange",
          "yhat.rf" = "purple",
          "yhat.xgb" = "pink",
          "yhat.h2o" = "yellow",
          "yhat.h2o_limvars" = "lightblue"
          )

lne_ <- c("solid",
          "dashed",
          "dashed",
          "dashed",
          "dashed",
          "dashed",
          "dashed",
          "dashed",
          "dashed"
          )


plot_actuals_vs_models <-
  forecast_data_to_plot_list %>%  
  map(~ .x %>% 
        mutate(l_type = if_else(key == "actual",
                                "actual",
                                "forecast"
                                )#,
               # l_size = if_else(key == "actual",
               #                  1.5,
               #                  1
               #                  )
               ) %>% 
        ggplot(aes(x = el_date,
                   y = el_rides,
                   color = key#,
                   # group = l_type
                   )
               ) +
        # facet_wrap(~ full_url,
        #            scales = "free"
        #            ) +
        geom_point(na.rm = TRUE) +
        geom_line(aes(linetype = l_type
                      ),
                  na.rm = TRUE
                  ) +
        scale_colour_manual(values = clrs
                            ) +
        scale_linetype_manual(values = lne_,
                              guide = FALSE
                              ) +
        # scale_size_manual(values = c(4, 1, 1, 1, 1)
        #                   ) +
        scale_x_date(limits = c(max(.$el_date) - (7 * 4),
                                max(.$el_date) + (7 * 1)
                                )
                     ) +
        ggtitle(label = .$el_stop_id) +
        theme_bw() +
        theme(legend.position = "bottom")
      )

plot_actuals_vs_models


rm(clrs, lne_, forecast_data_to_plot_list, func_get_forecast_data)

```
 
 
 
 

```{r}

library("lime")

local_obs <-
  # h2o_train_tbl$`40600` %>%
  # DV_nzv_predict$`40600` %>% 
  h2o.limvars_train_tbl$`40600` %>%
  filter(data.table::between(x = el_date,
                             # lower = as_date("2015-12-21"),
                             lower = as_date("2015-12-30"),
                             upper = as_date("2016-01-04"),
                             incbounds = TRUE
                             )
         )

# View(h2o_train_tbl$`40600`)
# View(DV_nzv_predict$`40600`)
h2o_explainer <- 
  lime::lime(
             # x = h2o_train_tbl$`40600`,
             # x = DV_nzv_predict$`40600`,
             x = h2o.limvars_train_tbl$`40600`,
             # model = h2o_automl_leader$`40600`,
             # model = DV_Fit.Rf.corr_no$`40600`,
             model = h2o.limvars_automl_leader$`40600`,
             n_bins = 5
             )

h2o_explanation <-
  lime::explain(local_obs,
                h2o_explainer,
                n_features = 5,
                kernel_width = .1,
                feature_select  = "highest_weights"
                # feature_select = "lasso_path"
                )

# h2o_explanation

plot_features(h2o_explanation)
plot_explanations(h2o_explanation)

```




 
```{r}

rm(list = ls(pattern = "h2o"))
rm(list = ls(pattern = "get"))
rm(list = ls(pattern = "func"))
rm(list = ls(pattern = "fit"))
rm(list = ls(pattern = "time."))
rm(list = ls(pattern = "DV"))
rm(list = ls(pattern = "models"))
rm(list = ls(pattern = "Models"))
rm(list = ls(pattern = "prophet"))
rm(list = ls(pattern = "Resample"))
rm(list = ls(pattern = "plot"))
rm(list = ls(pattern = "train"))
rm(list = ls(pattern = "trn"))
rm(holiday_dates, rds_files, roll_rs, x, y)

```



